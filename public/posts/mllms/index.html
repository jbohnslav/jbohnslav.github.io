<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>MLLMs, VLMs, LVLMs, LMMs... | Jim Robinson-Bohnslav</title>
<meta name="keywords" content="mllm, vlm, lvlm, lmm">
<meta name="description" content="What do we call these things?">
<meta name="author" content="James Robinson-Bohnslav">
<link rel="canonical" href="http://localhost:1313/posts/mllms/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk&#43;4bvpN&#43;sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/mllms/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/posts/mllms/">
  <meta property="og:site_name" content="Jim Robinson-Bohnslav">
  <meta property="og:title" content="MLLMs, VLMs, LVLMs, LMMs...">
  <meta property="og:description" content="What do we call these things?">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-11T21:11:57-05:00">
    <meta property="article:modified_time" content="2024-12-11T21:11:57-05:00">
    <meta property="article:tag" content="Mllm">
    <meta property="article:tag" content="Vlm">
    <meta property="article:tag" content="Lvlm">
    <meta property="article:tag" content="Lmm">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MLLMs, VLMs, LVLMs, LMMs...">
<meta name="twitter:description" content="What do we call these things?">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "MLLMs, VLMs, LVLMs, LMMs...",
      "item": "http://localhost:1313/posts/mllms/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "MLLMs, VLMs, LVLMs, LMMs...",
  "name": "MLLMs, VLMs, LVLMs, LMMs...",
  "description": "What do we call these things?",
  "keywords": [
    "mllm", "vlm", "lvlm", "lmm"
  ],
  "articleBody": "There exists a class of models whose inputs are text prompts + images or video. Their outputs are text.\nExample: “Explain the joke in this tweet. Be concise.” Answer, courtesy of GPT4o:\nThe joke humorously compares “the talk” about sensitive topics with explaining to kids why there’s a server at home. The mock children’s book title exaggerates the idea, poking fun at tech enthusiasts whose home servers are significant enough to require a formal explanation to their kids.\nBuilding these models is one of the biggest fields of both industrial AI and academic computer vision research. But no one can agree on what to call them! For the rest of this post I’m going to keep calling them “these models” because I don’t know what else to do.\nChoices Multimodal Large Language Models (MLLM)\nThis is probably the most common name for “these models”. Points in favor: they all use LLMs as a core component. These models generate text, just like LLMs. They are multi-modal, able to process multiple types of inputs.\nThe problem is that this name is not specific enough: imagine a model where you submit an audio file to and ask a question, e.g. “What sound is this?” + sound.wav -\u003e “This is a siren of an emergency vehicle.” Such a model would also be an MLLM.\nProponents: vLLM, Llama 31, Pixtral2, Gemini3, Waymo4, Cambrian-15, InternVL2.5 6, Mammoth-VL7, Florence-VL 8, Fei Fei Li (NeurIPS)\nVision-language model (VLM)\nVLM is more specific than MLLM, which is good. However, models like CLIP and SigLIP are Vision-Language Models too. They have image encoders, text encoders, can be prompted, etc. But CLIP et al. are not generative; they do not produce text. That makes this term confusing to me.\nProponents: Molmo9, Huggingface (SmolVLM), PaliGemma 2 10, CogVLM11, NVILA12\nLarge vision-language model (LVLM)\n“These models” are large, use vision, and generate language. Pretty good. But InternViT-6B is a CLIP-style model with 6 billion parameters: it’s large by any measure. InternViT isn’t generative, so it’s not the kind of model I mean. This paper13 even calls CLIP a VLM and “these models” LVLMs, so I guess generating text is what makes it “Large”?\nProponents: Qwen2-VL14\nLarge Multimodal Model (LMM)\nThis one is popular with the Llava folks. They get extra credit because their paper defined the field, but I see this as just a variant of MLLM.\nProponents: Llava15, Llava-OneVision16\nMy take I’ve been a proponent of calling “these models” MLLMs. However, models like GPT4o and Gemini Flash 2.0 can consume text, images, video, or audio, and generate text, images, or audio as well. That is truly multimodal. It’s such a big difference that the GPT4o system card17 calls it an “omni model”.\nModels focusing on images and videos specifically are going to be extremely valuable in many domains: robotics, web agents, as components in coding assistants, and in consumer apps. It therefore makes sense to define them as a class distinctly from the “omni models.”\nThrough writing this post, I’ve convinced myself that VLM is a more specific, useful term. With great apologies to Lucas Beyer and the rest of the SigLIP team, I will call models that learn a joint embedding space between images and text “CLIP-style models.”\nWhat do you think we should call VLMs? Let’s discuss on Twitter or BlueSky.\nReferences A. Dubey et al., “The Llama 3 Herd of Models,” Jul. 31, 2024, arXiv: arXiv:2407.21783. Accessed: Aug. 01, 2024. [Online]. Available: http://arxiv.org/abs/2407.21783 ↩︎\nP. Agrawal et al., “Pixtral 12B,” Oct. 09, 2024, arXiv: arXiv:2410.07073. Accessed: Oct. 10, 2024. [Online]. Available: http://arxiv.org/abs/2410.07073 ↩︎\nG. Team et al., “Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,” Aug. 08, 2024, arXiv: arXiv:2403.05530. doi: 10.48550/arXiv.2403.05530. ↩︎\nJ.-J. Hwang et al., “EMMA: End-to-End Multimodal Model for Autonomous Driving,” Oct. 30, 2024, arXiv: arXiv:2410.23262. Accessed: Nov. 04, 2024. [Online]. Available: http://arxiv.org/abs/2410.23262 ↩︎\nS. Tong et al., “Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs,” Jun. 24, 2024, arXiv: arXiv:2406.16860. Accessed: Jun. 25, 2024. [Online]. Available: http://arxiv.org/abs/2406.16860 ↩︎\nZ. Chen et al., “Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling,” Dec. 06, 2024, arXiv: arXiv:2412.05271. doi: 10.48550/arXiv.2412.05271. ↩︎\nJ. Guo et al., “MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale,” Dec. 06, 2024, arXiv: arXiv:2412.05237. doi: 10.48550/arXiv.2412.05237. ↩︎\nJ. Chen et al., “Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion,” Dec. 05, 2024, arXiv: arXiv:2412.04424. doi: 10.48550/arXiv.2412.04424. ↩︎\nM. Deitke et al., “Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models,” Sep. 25, 2024, arXiv: arXiv:2409.17146. Accessed: Sep. 26, 2024. [Online]. Available: http://arxiv.org/abs/2409.17146 ↩︎\nA. Steiner et al., “PaliGemma 2: A Family of Versatile VLMs for Transfer,” Dec. 04, 2024, arXiv: arXiv:2412.03555. doi: 10.48550/arXiv.2412.03555. ↩︎\nW. Hong et al., “CogVLM2: Visual Language Models for Image and Video Understanding,” Aug. 29, 2024, arXiv: arXiv:2408.16500. Accessed: Aug. 30, 2024. [Online]. Available: http://arxiv.org/abs/2408.16500 ↩︎\nZ. Liu et al., “NVILA: Efficient Frontier Visual Language Models,” Dec. 05, 2024, arXiv: arXiv:2412.04468. doi: 10.48550/arXiv.2412.04468. ↩︎\nY. Ouali et al., “Discriminative Fine-tuning of LVLMs,” Dec. 05, 2024, arXiv: arXiv:2412.04378. doi: 10.48550/arXiv.2412.04378. ↩︎\nP. Wang et al., “Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution,” Sep. 18, 2024, arXiv: arXiv:2409.12191. Accessed: Sep. 19, 2024. [Online]. Available: http://arxiv.org/abs/2409.12191 ↩︎\nH. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual Instruction Tuning,” Dec. 11, 2023, arXiv: arXiv:2304.08485. Accessed: Jun. 28, 2024. [Online]. Available: http://arxiv.org/abs/2304.08485 ↩︎\nB. Li et al., “LLaVA-OneVision: Easy Visual Task Transfer,” Aug. 06, 2024, arXiv: arXiv:2408.03326. Accessed: Aug. 07, 2024. [Online]. Available: http://arxiv.org/abs/2408.03326 ↩︎\n↩︎ ",
  "wordCount" : "907",
  "inLanguage": "en",
  "datePublished": "2024-12-11T21:11:57-05:00",
  "dateModified": "2024-12-11T21:11:57-05:00",
  "author":{
    "@type": "Person",
    "name": "James Robinson-Bohnslav"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/mllms/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jim Robinson-Bohnslav",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Jim Robinson-Bohnslav (Alt + H)">Jim Robinson-Bohnslav</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/about/" title="about">
                    <span>about</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="archives">
                    <span>archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      MLLMs, VLMs, LVLMs, LMMs...
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-description">
      What do we call these things?
    </div>
    <div class="post-meta"><span title='2024-12-11 21:11:57 -0500 EST'>December 11, 2024</span>&nbsp;·&nbsp;James Robinson-Bohnslav

</div>
  </header> 
  <div class="post-content"><p>There exists a class of models whose <em>inputs are text prompts + images or video. Their outputs are text.</em></p>
<p>Example:  &ldquo;Explain the joke in this tweet. Be concise.&rdquo;
<figure>
    <img loading="lazy" src="/images/server-joke.webp"
         alt="home-server-joke" width="600"/> 
</figure>
</p>
<p>Answer, courtesy of GPT4o:</p>
<blockquote>
<p>The joke humorously compares &ldquo;the talk&rdquo; about sensitive topics with explaining to kids why there&rsquo;s a server at home. The mock children&rsquo;s book title exaggerates the idea, poking fun at tech enthusiasts whose home servers are significant enough to require a formal explanation to their kids.</p>
</blockquote>
<p>Building these models is one of the biggest fields of both industrial AI and academic computer vision research. But no one can agree on what to call them! For the rest of this post I&rsquo;m going to keep calling them &ldquo;these models&rdquo; because I don&rsquo;t know what else to do.</p>
<h2 id="choices">Choices<a hidden class="anchor" aria-hidden="true" href="#choices">#</a></h2>
<p><strong>Multimodal Large Language Models (MLLM)</strong></p>
<p>This is probably the most common name for &ldquo;these models&rdquo;. Points in favor: they all use LLMs as a core component. These models <em>generate text</em>, just like LLMs. They are multi-modal, able to process multiple types of inputs.</p>
<p>The problem is that this name is not specific enough: imagine a model where you submit an audio file to and ask a question, e.g. &ldquo;What sound is this?&rdquo;  + <code>sound.wav</code> -&gt; &ldquo;This is a siren of an emergency vehicle.&rdquo; Such a model would also be an MLLM.</p>
<p>Proponents: <a href="https://docs.vllm.ai/en/latest/usage/multimodal_inputs.html#multimodal-inputs">vLLM</a>, Llama 3<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, Pixtral<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, Gemini<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, Waymo<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>, Cambrian-1<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>, InternVL2.5 <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>, Mammoth-VL<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>, Florence-VL <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>, Fei Fei Li (NeurIPS)</p>
<p><strong>Vision-language model (VLM)</strong></p>
<p>VLM is more specific than MLLM, which is good. However, models like CLIP and SigLIP are Vision-Language Models too. They have image encoders, text encoders, can be prompted, etc. But CLIP et al. are not <em>generative</em>; they do not produce text. That makes this term confusing to me.</p>
<p>Proponents: Molmo<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>, <a href="https://huggingface.co/blog/smolvlm">Huggingface (SmolVLM)</a>, PaliGemma 2 <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>, CogVLM<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, NVILA<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup></p>
<p><strong>Large vision-language model (LVLM)</strong></p>
<p>&ldquo;These models&rdquo; are large, use vision, and generate language. Pretty good. But InternViT-6B is a CLIP-style model with 6 billion parameters: it&rsquo;s large by any measure. InternViT isn&rsquo;t generative, so it&rsquo;s not the kind of model I mean. This paper<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> even calls CLIP a VLM and &ldquo;these models&rdquo; LVLMs, so I guess generating text is what makes it &ldquo;Large&rdquo;?</p>
<p>Proponents: Qwen2-VL<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup></p>
<p><strong>Large Multimodal Model (LMM)</strong></p>
<p>This one is popular with the Llava folks. They get extra credit because their paper defined the field, but I see this as just a variant of MLLM.</p>
<p>Proponents: Llava<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>, Llava-OneVision<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup></p>
<h2 id="my-take">My take<a hidden class="anchor" aria-hidden="true" href="#my-take">#</a></h2>
<p>I&rsquo;ve been a proponent of calling &ldquo;these models&rdquo; MLLMs. However, models like GPT4o and Gemini Flash 2.0 can consume text, images, video, or audio, and generate text, images, or audio as well. <em>That</em> is truly multimodal. It&rsquo;s such a big difference that the GPT4o system card<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> calls it an &ldquo;omni model&rdquo;.</p>
<p>Models focusing on images and videos specifically are going to be extremely valuable in many domains: robotics, web agents, as components in coding assistants, and in consumer apps. It therefore makes sense to define them as a class distinctly from the &ldquo;omni models.&rdquo;</p>
<p>Through writing this post, I&rsquo;ve convinced myself that VLM is a more specific, useful term. With great apologies to Lucas Beyer and the rest of the SigLIP team, I will call models that learn a joint embedding space between images and text &ldquo;CLIP-style models.&rdquo;</p>
<p>What do you think we should call VLMs? Let&rsquo;s discuss on Twitter or BlueSky.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>A. Dubey <em>et al.</em>, &ldquo;The Llama 3 Herd of Models,&rdquo; Jul. 31, 2024, <em>arXiv</em>: arXiv:2407.21783. Accessed: Aug. 01, 2024. [Online]. Available: <a href="http://arxiv.org/abs/2407.21783">http://arxiv.org/abs/2407.21783</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>P. Agrawal <em>et al.</em>, &ldquo;Pixtral 12B,&rdquo; Oct. 09, 2024, <em>arXiv</em>: arXiv:2410.07073. Accessed: Oct. 10, 2024. [Online]. Available: <a href="http://arxiv.org/abs/2410.07073">http://arxiv.org/abs/2410.07073</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>G. Team <em>et al.</em>, &ldquo;Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,&rdquo; Aug. 08, 2024, <em>arXiv</em>: arXiv:2403.05530. doi: <a href="https://doi.org/10.48550/arXiv.2403.05530">10.48550/arXiv.2403.05530</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>J.-J. Hwang <em>et al.</em>, &ldquo;EMMA: End-to-End Multimodal Model for Autonomous Driving,&rdquo; Oct. 30, 2024, <em>arXiv</em>: arXiv:2410.23262. Accessed: Nov. 04, 2024. [Online]. Available: <a href="http://arxiv.org/abs/2410.23262">http://arxiv.org/abs/2410.23262</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>S. Tong <em>et al.</em>, &ldquo;Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs,&rdquo; Jun. 24, 2024, <em>arXiv</em>: arXiv:2406.16860. Accessed: Jun. 25, 2024. [Online]. Available: <a href="http://arxiv.org/abs/2406.16860">http://arxiv.org/abs/2406.16860</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Z. Chen <em>et al.</em>, &ldquo;Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling,&rdquo; Dec. 06, 2024, <em>arXiv</em>: arXiv:2412.05271. doi: <a href="https://doi.org/10.48550/arXiv.2412.05271">10.48550/arXiv.2412.05271</a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>J. Guo <em>et al.</em>, &ldquo;MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale,&rdquo; Dec. 06, 2024, <em>arXiv</em>: arXiv:2412.05237. doi: <a href="https://doi.org/10.48550/arXiv.2412.05237">10.48550/arXiv.2412.05237</a>.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>J. Chen <em>et al.</em>, &ldquo;Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion,&rdquo; Dec. 05, 2024, <em>arXiv</em>: arXiv:2412.04424. doi: <a href="https://doi.org/10.48550/arXiv.2412.04424">10.48550/arXiv.2412.04424</a>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>M. Deitke <em>et al.</em>, &ldquo;Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models,&rdquo; Sep. 25, 2024, <em>arXiv</em>: arXiv:2409.17146. Accessed: Sep. 26, 2024. [Online]. Available: <a href="http://arxiv.org/abs/2409.17146">http://arxiv.org/abs/2409.17146</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>A. Steiner <em>et al.</em>, &ldquo;PaliGemma 2: A Family of Versatile VLMs for Transfer,&rdquo; Dec. 04, 2024, <em>arXiv</em>: arXiv:2412.03555. doi: <a href="https://doi.org/10.48550/arXiv.2412.03555">10.48550/arXiv.2412.03555</a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>W. Hong <em>et al.</em>, &ldquo;CogVLM2: Visual Language Models for Image and Video Understanding,&rdquo; Aug. 29, 2024, <em>arXiv</em>: arXiv:2408.16500. Accessed: Aug. 30, 2024. [Online]. Available: <a href="http://arxiv.org/abs/2408.16500">http://arxiv.org/abs/2408.16500</a>&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Z. Liu <em>et al.</em>, &ldquo;NVILA: Efficient Frontier Visual Language Models,&rdquo; Dec. 05, 2024, <em>arXiv</em>: arXiv:2412.04468. doi: <a href="https://doi.org/10.48550/arXiv.2412.04468">10.48550/arXiv.2412.04468</a>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Y. Ouali <em>et al.</em>, &ldquo;Discriminative Fine-tuning of LVLMs,&rdquo; Dec. 05, 2024, <em>arXiv</em>: arXiv:2412.04378. doi: <a href="https://doi.org/10.48550/arXiv.2412.04378">10.48550/arXiv.2412.04378</a>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>P. Wang <em>et al.</em>, &ldquo;Qwen2-VL: Enhancing Vision-Language Model&rsquo;s Perception of the World at Any Resolution,&rdquo; Sep. 18, 2024, <em>arXiv</em>: arXiv:2409.12191. Accessed: Sep. 19, 2024. [Online]. Available: <a href="http://arxiv.org/abs/2409.12191">http://arxiv.org/abs/2409.12191</a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>H. Liu, C. Li, Q. Wu, and Y. J. Lee, &ldquo;Visual Instruction Tuning,&rdquo; Dec. 11, 2023, <em>arXiv</em>: arXiv:2304.08485. Accessed: Jun. 28, 2024. [Online]. Available: <a href="http://arxiv.org/abs/2304.08485">http://arxiv.org/abs/2304.08485</a>&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>B. Li <em>et al.</em>, &ldquo;LLaVA-OneVision: Easy Visual Task Transfer,&rdquo; Aug. 06, 2024, <em>arXiv</em>: arXiv:2408.03326. Accessed: Aug. 07, 2024. [Online]. Available: <a href="http://arxiv.org/abs/2408.03326">http://arxiv.org/abs/2408.03326</a>&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/mllm/">Mllm</a></li>
      <li><a href="http://localhost:1313/tags/vlm/">Vlm</a></li>
      <li><a href="http://localhost:1313/tags/lvlm/">Lvlm</a></li>
      <li><a href="http://localhost:1313/tags/lmm/">Lmm</a></li>
    </ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="http://localhost:1313/">Jim Robinson-Bohnslav</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
