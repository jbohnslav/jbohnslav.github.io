<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Defending Muon: A Deep Dive into Moonshot&#39;s K2 Optimizer (A Translated Analysis) | Jim Robinson-Bohnslav</title>
<meta name="keywords" content="translation, muon, kimi2, moonshot, optimizer, ai, technical">
<meta name="description" content="Translation of a detailed technical analysis defending Moonshot AI&#39;s Muon optimizer used in their K2 model">
<meta name="author" content="toothacher17">
<link rel="canonical" href="http://localhost:1313/posts/muon-kimi2-translation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.1d79e034ed3fae3cce466e1b13b697315f9998d1ba8d908def178611b372b49f.css" integrity="sha256-HXngNO0/rjzORm4bE7aXMV&#43;ZmNG6jZCN7xeGEbNytJ8=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/muon-kimi2-translation/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-VWMGL11SYN"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-VWMGL11SYN');
        }
      </script><meta property="og:url" content="http://localhost:1313/posts/muon-kimi2-translation/">
  <meta property="og:site_name" content="Jim Robinson-Bohnslav">
  <meta property="og:title" content="Defending Muon: A Deep Dive into Moonshot&#39;s K2 Optimizer (A Translated Analysis)">
  <meta property="og:description" content="Translation of a detailed technical analysis defending Moonshot AI&#39;s Muon optimizer used in their K2 model">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-07-12T14:05:31-04:00">
    <meta property="article:modified_time" content="2025-07-12T14:05:31-04:00">
    <meta property="article:tag" content="Translation">
    <meta property="article:tag" content="Muon">
    <meta property="article:tag" content="Kimi2">
    <meta property="article:tag" content="Moonshot">
    <meta property="article:tag" content="Optimizer">
    <meta property="article:tag" content="Ai">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Defending Muon: A Deep Dive into Moonshot&#39;s K2 Optimizer (A Translated Analysis)">
<meta name="twitter:description" content="Translation of a detailed technical analysis defending Moonshot AI&#39;s Muon optimizer used in their K2 model">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Defending Muon: A Deep Dive into Moonshot's K2 Optimizer (A Translated Analysis)",
      "item": "http://localhost:1313/posts/muon-kimi2-translation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Defending Muon: A Deep Dive into Moonshot's K2 Optimizer (A Translated Analysis)",
  "name": "Defending Muon: A Deep Dive into Moonshot\u0027s K2 Optimizer (A Translated Analysis)",
  "description": "Translation of a detailed technical analysis defending Moonshot AI's Muon optimizer used in their K2 model",
  "keywords": [
    "translation", "muon", "kimi2", "moonshot", "optimizer", "ai", "technical"
  ],
  "articleBody": "About the translation This is a translation of the original blog post by toothacher17. The original post is in Chinese and can be found here. The author’s tweet about it is here. I (Jim Robinson-Bohnslav) translated it using Google Translate, Deepseek-R1, Gemini 2.5 Pro, and O3.\nOriginal Post Author: toothacher17\nOriginal Link: https://www.zhihu.com/question/1927140506573435010/answer/1927378524513219780\nSource: Zhihu\nCopyright belongs to the author. For commercial reprint, please contact the author for authorization. For non-commercial reprint, please indicate the source.\nDisclaimer: Former Moonshot “No. 1 hype-man” (some say I’m competing with @Andrew Lu) and long-time Feilai Pavilion fan1—just riding the K2 hype wave.\n1. Concerns About Using the Muon Optimizer It’s worth noting that the K2 model released by Moonshot was trained end-to-end using the Muon optimizer2 3. Muon was first proposed by Keller and performed exceptionally well in Speedrun2. It was then picked up by Moonshot, where they made some adjustments and scaled it up3 4.\nIn Moonshot’s early work3 they highlighted Muon’s impressive token-efficiency and even released a Megatron-LM implementation5. Subsequent discussion on X (formerly Twitter) surfaced three recurring concerns:\nExpensive Operations: Muon requires the full parameter matrix for its “Normalized Stochasticity” (NS) calculation. In the parallel setting of modern LLM training infrastructure, many believe operating on the full parameter matrix is too expensive. Complex Hyperparameter Tuning: Muon requires “several sets” of different hyperparameter tuning mechanisms, which places higher demands on model tuning. In contrast, self-adaptive optimizers like AdamW seem simpler and more stable to tune. Training Instability: Muon might cause training instability. For instance, Moonshot’s own paper3 mentioned a potential issue with the attention max logit. In fact, with the release of K26, it’s clear these problems aren’t deal-breakers. This blog post will attempt to “argue the case” for why.\n2. Concern 1: Muon’s Infrastructure Scalability First, let’s discuss whether operating on Muon’s full parameter matrix is truly expensive, and in doing so, fill in a small gap left in the previous paper3.\nTo clarify this, we need to detail Zero-1 sharding. Then, by understanding its implementation and the differences between Chinese and international training clusters (Why international? Because some foreign companies are challenging this on X, essentially because they are flush with cash and have too many GPUs), we can explain why the Moonshot team believes Muon’s infra is scalable, while others remain skeptical.\n2.1 Zero-1 Sharding First, some background. In modern LLM training involving large models and large clusters, the Zero-1 optimizer is a standard technique. Frameworks like Megatron-LM, DeepSpeed, and FSDP all have support for it.\nZero-1 technology essentially shards the optimizer states—which consume a lot of GPU memory (e.g., AdamW’s exp, exp_square, fp32_master_weights)—across the Data Parallel (DP) group.\nWhen using AdamW, the lifecycle of the Zero-1 Distributed Optimizer is as follows:\nGradient Reduce-Scatter: Perform a reduce_scatter of gradients between DP ranks. It’s a reduce_scatter instead of an all_reduce because of the sharding. Each DP rank only needs to ensure the gradients for the local parameters it’s responsible for are accurate. Local Parameter Update: Perform the AdamW update calculation for the local parameters. Since AdamW’s calculation is element-wise, this step only needs to compute the updates for local parameters. Parameter All-Gather: Perform a parameter all_gather between DP ranks. Because each DP rank only updated a portion of the parameters, an all_gather is needed for all ranks to get the complete, updated set of parameters. Note that steps 1 and 3, while seemingly communication-heavy, can actually be overlapped with the model’s forward/backward pass (a very mature technique all major frameworks implement), so there’s no need to worry. In step 2, since AdamW is element-wise and the computation per rank decreases as DP size increases, it’s highly scalable.\nIn summary, this distributed optimizer technology is very friendly to AdamW. The time cost of the AdamW optimizer is typically less than 1% of the entire global step, basically negligible compared to the forward/backward pass.\nHowever, Muon faces a significant challenge in step 2 because its calculation is not element-wise. Muon requires the full parameter matrix to compute NS, which inevitably introduces additional communication and a larger computational load (running NS on the entire matrix).\nFor Muon to be as scalable as possible, the communication overhead of step 2 needs to be minimal (as it can hardly be hidden), and the additional computation introduced needs to be as small as possible (a single small matrix runs NS quickly, so we should avoid running NS on overly large or numerous matrices per DP rank).\n2.2 The Moonshot Solution Based on Moonshot’s open-source work5, it’s speculated that their development is based on a version of Megatron-LM that they have since maintained. For Megatron-LM, its early implementation of the Zero-1 optimizer7 is as follows (we’ll call it “flat-param concat zero-1”):\nAs you can see, the approach is to flatten all optimizer states, concatenate them, and then distribute them evenly across the DP group. This allocation method is optimal for GPU memory because there are no duplicate optimizer states. Moreover, this partitioning is highly beneficial for Muon because most of the local parameters remain complete and can be directly used for the NS operation. Only the parameters at the DP boundaries are split across two DP ranks and become incomplete, requiring special handling.\nSpecifically, taking DP0 and DP1 jointly processing Param 1 as an example, if we were to brainstorm solutions, there are several approaches:\nThe “Brainless” Gather Method: DP0 and DP1 each perform a gather to get the full parameters. Both ranks then perform the full NS calculation. After computation, each rank only updates its local portion of the parameters and discards the rest. The grad_reduce_scatter and params_all_gather of steps 1 and 3 remain unchanged to avoid redesigning the algorithm. Edge Parameter Passing: Each DP rank i sends its edge parameters to DP i-1. DP i-1 is then responsible for the computation on these edge parameters. After calculation, the result is sent back to rank i to update the portion it maintains. This avoids redundant computation, and the communication volume is actually better than the brainless gather method. However, for extreme cases, like a parameter spanning three DP ranks, this requires more complex heuristic arrangements. Heuristic Precision Arrangement: When arranging the distributed optimizer, prevent the DP edge-splitting from happening in the first place. This eliminates any extra communication and computation. The cost is that memory allocation is no longer balanced, and finding the optimal allocation becomes a knapsack problem. Unbalanced memory allocation is obviously unacceptable for infrastructure engineers as it leads to inaccurate memory estimation during training, affecting the parallel allocation strategy. In practice, Moonshot uses the brainless gather method because it is the simplest to implement and covers all edge cases. Crucially, the overhead is small—only parameters that straddle a DP boundary (≈ DP × 2) incur duplicate computation or extra communication. Other parameters, like param0 and param2 in the diagram, are complete and don’t require any extra work.\nEmpirically, the actual performance of this communication and computation will be affected by the number of DP ranks and the maximum matrix size in the model. Considering modern MoE architectures (thanks, DeepSeek-V2), a model won’t have excessively large matrices because they are all fine-grained experts (and word embedding/lm_head are controlled by AdamW, not Muon). Therefore, in the long run, Muon’s scalability has a bright future and is steadily improving.\nSince the cost of the brainless method is already low, the benefits of engineering a more complex solution are minimal, which is likely why “Jiang Kernel” (a nickname for a key person) didn’t have the motivation to pursue it further (though I recall You Jiacheng might have implemented some similar solutions on Speedrun?).\n2.3 Others’ Concerns However, in the research from some foreign companies, there is a pessimistic bias towards Muon’s scalability8 9 10 11, and Moonshot’s method5 has been repeatedly criticized. Obviously, it’s not that everyone else is an idiot. But based on the analysis in 2.2 and the fact that Moonshot successfully trained K2 at a large scale, Moonshot isn’t an idiot either.\nI personally believe the main reason for this conflict is the different implementations of Zero-1, which leads to a large discrepancy in the estimated overhead of Step 2.\nThe mainstream method abroad is called dim-0 sharding Zero-1. For example, the Zero-1 implementation in the mainstream foreign parallel framework, PyTorch FSDP2, is as follows12:\nAnd a newer version of Megatron-LM13 introduced the concept of “buckets.” The essence of this concept is similar in effect to params dim-0 sharding:\nThese updates are actually a “devastating” blow to the Muon implementation that preceded Moonshot’s work. This type of Zero-1 implementation causes every parameter to be sharded by DP! Whether it’s the brainless gather method, the edge-passing method, or the sophisticated arrangement method, all of which are based on “flat-param concat zero-1,” they are all ruined. Every parameter now requires communication and redundant recalculation, leading to a massive amount of extra overhead, making Muon unacceptable.\n2.4 Long-Term Solution Foreign companies are definitely not stupid. Early parallel designs actually all used flat-param concat zero-114. Later, due to other concerns (mainly that foreign companies have too many GPUs, and flat params are not conducive to overlapping grad_reduce_scatter and params_all_gather), they switched to dim-0 params sharding Zero-1.\nIn the context of mandatory dim-0 params sharding, the Moonshot method is indeed not scalable. But this does not mean Muon is inherently unscalable. New solutions will definitely emerge. In fact, I’ve heard that it seems possible, and someone might already be working on it (smirking dog face emoji).\n3. Concern 2: Muon Needs More Hyperparameters Another common complaint is that Muon has several sets of hyperparameters, which is seen as a significant disadvantage compared to AdamW:\nIt requires additional tuning efforts. The need for extra tuning means more mental overhead to find the best model, which isn’t a fair comparison to AdamW. If AdamW were also tuned in blocks, it might achieve better results. I personally think this concern stems from a lack of precise understanding of the mathematical properties of the Muon optimizer. To understand Muon, we need to look at it from the perspectives of Standard Parametrization (SP) and Maximal Update Parametrization (µP) to see why multiple sets of parameters need adjustment.\nAdditionally, Muon is designed for matrices2. Non-matrix parameters like word embeddings, lm_head, and rmsnorm_gamma are all updated using AdamW.\n3.1 Standard Parametrization (SP) + Muon Let’s first look at Muon under SP. When Moonshot started researching/reproducing (i.e., copying) Keller’s Muon in its early days (around January 2024)15, it looked like this (without weight decay and without the various engineering optimizations added by Mr. You, like the zero-1 optimizations):\nAt this stage, there weren’t so many outrageous sets of parameters—just one set for AdamW and one for Muon. However, the update RMS (Root Mean Square) of Muon is very different from that of AdamW. In Moonshot’s work3, Su Yin provided a derivation:\nThis shows that AdamW’s update RMS is empirically around 0.2-0.4, while Muon’s is much smaller. If you don’t increase Muon’s update RMS (the simplest way being a dedicated learning rate), Muon simply won’t update effectively, making it an unfair comparison.\nIn the SP setting, if you don’t want to tune two sets of parameters, you can directly use Moonshot’s work3. By matching the update RMS, it’s practically “out-of-the-box.” You can use a single set of AdamW hyperparameters. There’s plenty of work on how to tune AdamW hyperparameters (e.g., the stepfun law). Just copy one and migrate it to Muon using Moonshot’s method, and you will likely get good improved loss token efficiency.\nIn fact, the main contribution of Moonshot’s work is here: allowing everyone to migrate to Muon in the SP setting without much thought. My superficial understanding is that this is equivalent to the fastest optimization under a matrix Frobenius norm constraint, which effectively controls the update RMS, similar to AdamW. It meets the requirements of SP, but it’s not optimal. For Muon, the theoretically optimal method is the fastest optimization under a spectral norm constraint, which we will discuss next.\n3.2 µP Parametrization + Muon The most exciting use of Muon is not SP, but its combination with µP (Maximal Update Parametrization). A series of open-source works have provided very exciting introductions! [^16 ]16 17.\nIn short, Muon is almost an optimizer tailor-made for µP. Unlike using µP + AdamW, which introduces many variance-based assumptions, Muon naturally controls the spectral norm (because NS mathematically clips the max singular values, and the max singular value is the spectral norm by definition). This makes it perfectly suited for the spectral norm control required by high-order µP16!\nLooking at Keller’s improvement history on Muon, besides infrastructure optimizations by masters like Mr. You, the main evolution was the introduction of µP ideas by the “god-tier” Jeremy Bernstein (Jeremy is an author of both µP and the Muon blog, so he’s a double-threat).\nAfter introducing ideas similar to µP, the Embedding, LM Head, and Hidden Matrices all got their own control logic18. Although it seems outrageous, it’s reasonable when you consider the need to adapt to µP (in fact, adapting AdamW for µP also requires learning rate adjustments for different modules).\nIn particular, look at the adjustment of Muon’s update RMS here. Ignore the max(1, x) part for a moment and just look at the sqrt(d_out/d_in) part. This is exactly the same as the derivation in Su Yin’s high-order µP blog16! (Though I don’t know why the max(1, x) operation was added. With max, it actually reverts to a Frobenius norm-like scaling, doesn’t it?)\n4. Concern 3: Muon Training Instabilities In reality, few companies train Muon at truly large scale. Moonshot themselves report only two instability sources3 6:\nWeight decay. The max attention logit problem (addressed by muonclip). Weight decay is easy to understand, while the max attention logit problem involves the muonclip method mentioned in their recent blog6.\nThe max attention logit problem can usually be solved with qknorm, but Moonshot used MLA (Multi-Head Latent Attention) in K2 (I have to say, DeepSeek is ruthless; their model architectures are tried-and-true winners). The results are probably just that good, so there’s no need to force innovation when a great technology already exists. MLA adds normalization during compression, but for inference efficiency, the q and k heads aren’t materialized, which means you can’t perform qk-head normalization.\nTherefore, Moonshot took a different approach and created muonclip (in fact, others have also expressed concerns about the effectiveness of qknorm19).\nI personally find muonclip very elegant! In Su Yin’s high-order MuP blog16, we learn that the spectral norm is smaller than the Frobenius norm:\nAnd the spectral norm is directly tied to the maximum logit size, i.e.\n||x W||₂ ≤ ||x||₂ · ||W||₂\n(where W is a matrix, so ||W||₂ is its spectral norm). The most direct approach is to control the spectral norm. However, the spectral norm is difficult to calculate. So, we can use the inequality relationship between spectral and Frobenius norms and directly clip the Frobenius norm. By doing so, ||xW||_2 will be controlled!\nBut later I had a chance to chat with Su Yin, and he said he didn’t think that far ahead, and my understanding might not be right (I was floored). His idea was to directly operate on the fundamental problem. Su Yin mentioned he will be releasing a blog post in the next few days, so keep an eye out for that.\n5. Conclusion I feel that K2 is going to be a very powerful model, and I look forward to more evaluations from the community. Additionally, Moonshot has been very strong in Vision-Language (VL) and Reinforcement Learning (RL) before, so we can expect that after some more training, a K2-based model for thinking and vision understanding will have a chance to shine!\nAt the same time, as a company with many masters like Su Yin, “Jiang Kernel,” and Feilaige’s own Zhang Yu, Moonshot feels very promising! Moreover, Moonshot not only implements fancy new technologies like Muon but also generously acknowledges and uses advanced technologies from competitors. I feel that shows great character and vision!\nFootnotes The Story of Feilai Pavilion (Chinese) ↩︎\nKeller Jordan’s Muon Blog ↩︎ ↩︎ ↩︎\nMoonshot Muon Paper ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎\nWhy Use Muon (Chinese) ↩︎\nMegatron-LM PR for Muon ↩︎ ↩︎ ↩︎\nMoonshot K2 Announcement ↩︎ ↩︎ ↩︎\nMegatron-LM Zero-1 Sharding Scheme Image ↩︎\nKeller Jordan defending on X ↩︎\nEssential AI critiques Moonshot ↩︎\nDion’s critique of Moonshot ↩︎\nSeunghyun Seo’s critique of Moonshot on X ↩︎\nPyTorch FSDP2 Sharding Docs ↩︎\nMegatron-LM bucket implementation ↩︎\nPyTorch FSDP1 Flat Params ↩︎\nKeller’s early Muon implementation ↩︎\nHigh-order µP Derivations (Chinese) ↩︎ ↩︎ ↩︎ ↩︎\nDiscussion on X about Muon + µP ↩︎\nKeller’s latest Muon implementation ↩︎\nPost on X calling qknorm a “band-aid” ↩︎\n",
  "wordCount" : "2757",
  "inLanguage": "en",
  "datePublished": "2025-07-12T14:05:31-04:00",
  "dateModified": "2025-07-12T14:05:31-04:00",
  "author":{
    "@type": "Person",
    "name": "toothacher17"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/muon-kimi2-translation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jim Robinson-Bohnslav",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Jim Robinson-Bohnslav (Alt + H)">Jim Robinson-Bohnslav</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/about/" title="about">
                    <span>about</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="archives">
                    <span>archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Defending Muon: A Deep Dive into Moonshot&#39;s K2 Optimizer (A Translated Analysis)
    </h1>
    <div class="post-description">
      Translation of a detailed technical analysis defending Moonshot AI&#39;s Muon optimizer used in their K2 model
    </div>
    <div class="post-meta"><span title='2025-07-12 14:05:31 -0400 EDT'>July 12, 2025</span>&nbsp;·&nbsp;toothacher17

</div>
  </header> 
  <div class="post-content"><h2 id="about-the-translation">About the translation<a hidden class="anchor" aria-hidden="true" href="#about-the-translation">#</a></h2>
<p>This is a translation of the original blog post by toothacher17. The original post is in Chinese and can be found <a href="https://www.zhihu.com/question/1927140506573435010/answer/1927378524513219780">here</a>. The author&rsquo;s tweet about it is <a href="https://x.com/JingyuanLiu123/status/1944071538569097352">here</a>. I (Jim Robinson-Bohnslav) translated it using Google Translate, Deepseek-R1, Gemini 2.5 Pro, and O3.</p>
<h2 id="original-post">Original Post<a hidden class="anchor" aria-hidden="true" href="#original-post">#</a></h2>
<p><strong>Author:</strong> toothacher17</p>
<p><strong>Original Link:</strong> <a href="https://www.zhihu.com/question/1927140506573435010/answer/1927378524513219780">https://www.zhihu.com/question/1927140506573435010/answer/1927378524513219780</a></p>
<p><strong>Source:</strong> Zhihu</p>
<p><em>Copyright belongs to the author. For commercial reprint, please contact the author for authorization. For non-commercial reprint, please indicate the source.</em></p>
<p><em>Disclaimer: <em>Former</em> Moonshot “No. 1 hype-man” (some say I’m competing with @Andrew Lu) and long-time Feilai Pavilion fan<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>—just riding the K2 hype wave.</em></p>
<h2 id="1-concerns-about-using-the-muon-optimizer">1. Concerns About Using the Muon Optimizer<a hidden class="anchor" aria-hidden="true" href="#1-concerns-about-using-the-muon-optimizer">#</a></h2>
<p>It&rsquo;s worth noting that the K2 model released by Moonshot was trained end-to-end using the Muon optimizer<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Muon was first proposed by Keller and performed exceptionally well in Speedrun<sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. It was then picked up by Moonshot, where they made some adjustments and scaled it up<sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p><img alt="Muon in the Moonshot paper has huge advantages." loading="lazy" src="/images/blog_image_1.webp" title="Moonshot&#39;s paper highlighting the significant advantages of Muon."></p>
<p>In Moonshot’s early work<sup id="fnref2:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> they highlighted Muon’s impressive token-efficiency and even released a Megatron-LM implementation<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>.  Subsequent discussion on X (formerly Twitter) surfaced three recurring concerns:</p>
<ol>
<li><strong>Expensive Operations:</strong> Muon requires the full parameter matrix for its &ldquo;Normalized Stochasticity&rdquo; (NS) calculation. In the parallel setting of modern LLM training infrastructure, many believe operating on the full parameter matrix is too expensive.</li>
<li><strong>Complex Hyperparameter Tuning:</strong> Muon requires &ldquo;several sets&rdquo; of different hyperparameter tuning mechanisms, which places higher demands on model tuning. In contrast, self-adaptive optimizers like AdamW seem simpler and more stable to tune.</li>
<li><strong>Training Instability:</strong> Muon might cause training instability. For instance, Moonshot&rsquo;s own paper<sup id="fnref3:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> mentioned a potential issue with the attention max logit.</li>
</ol>
<p>In fact, with the release of K2<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>, it&rsquo;s clear these problems aren&rsquo;t deal-breakers. This blog post will attempt to &ldquo;argue the case&rdquo; for why.</p>
<h2 id="2-concern-1-muons-infrastructure-scalability">2. Concern 1: Muon&rsquo;s Infrastructure Scalability<a hidden class="anchor" aria-hidden="true" href="#2-concern-1-muons-infrastructure-scalability">#</a></h2>
<p>First, let&rsquo;s discuss whether operating on Muon&rsquo;s full parameter matrix is truly expensive, and in doing so, fill in a small gap left in the previous paper<sup id="fnref4:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p>To clarify this, we need to detail Zero-1 sharding. Then, by understanding its implementation and the differences between Chinese and international training clusters (Why international? Because some foreign companies are challenging this on X, essentially because they are flush with cash and have too many GPUs), we can explain why the Moonshot team believes Muon&rsquo;s infra is scalable, while others remain skeptical.</p>
<h3 id="21-zero-1-sharding">2.1 Zero-1 Sharding<a hidden class="anchor" aria-hidden="true" href="#21-zero-1-sharding">#</a></h3>
<p>First, some background. In modern LLM training involving large models and large clusters, the Zero-1 optimizer is a standard technique. Frameworks like Megatron-LM, DeepSpeed, and FSDP all have support for it.</p>
<p>Zero-1 technology essentially shards the optimizer states—which consume a lot of GPU memory (e.g., AdamW&rsquo;s <code>exp</code>, <code>exp_square</code>, <code>fp32_master_weights</code>)—across the Data Parallel (DP) group.</p>
<p>When using AdamW, the lifecycle of the Zero-1 Distributed Optimizer is as follows:</p>
<ol>
<li><strong>Gradient Reduce-Scatter:</strong> Perform a <code>reduce_scatter</code> of gradients between DP ranks. It&rsquo;s a <code>reduce_scatter</code> instead of an <code>all_reduce</code> because of the sharding. Each DP rank only needs to ensure the gradients for the local parameters it&rsquo;s responsible for are accurate.</li>
<li><strong>Local Parameter Update:</strong> Perform the AdamW update calculation for the local parameters. Since AdamW&rsquo;s calculation is element-wise, this step only needs to compute the updates for local parameters.</li>
<li><strong>Parameter All-Gather:</strong> Perform a parameter <code>all_gather</code> between DP ranks. Because each DP rank only updated a portion of the parameters, an <code>all_gather</code> is needed for all ranks to get the complete, updated set of parameters.</li>
</ol>
<p>Note that steps 1 and 3, while seemingly communication-heavy, can actually be overlapped with the model&rsquo;s forward/backward pass (a very mature technique all major frameworks implement), so there&rsquo;s no need to worry. In step 2, since AdamW is element-wise and the computation per rank decreases as DP size increases, it&rsquo;s highly scalable.</p>
<p>In summary, this distributed optimizer technology is very friendly to AdamW. The time cost of the AdamW optimizer is typically less than 1% of the entire global step, basically negligible compared to the forward/backward pass.</p>
<p>However, Muon faces a significant challenge in step 2 because its calculation is <em>not</em> element-wise. Muon requires the <strong>full parameter matrix</strong> to compute NS, which inevitably introduces additional communication and a larger computational load (running NS on the entire matrix).</p>
<p>For Muon to be as scalable as possible, the communication overhead of step 2 needs to be minimal (as it can hardly be hidden), and the additional computation introduced needs to be as small as possible (a single small matrix runs NS quickly, so we should avoid running NS on overly large or numerous matrices per DP rank).</p>
<h3 id="22-the-moonshot-solution">2.2 The Moonshot Solution<a hidden class="anchor" aria-hidden="true" href="#22-the-moonshot-solution">#</a></h3>
<p>Based on Moonshot&rsquo;s open-source work<sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>, it&rsquo;s speculated that their development is based on a version of Megatron-LM that they have since maintained. For Megatron-LM, its early implementation of the Zero-1 optimizer<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> is as follows (we&rsquo;ll call it &ldquo;flat-param concat zero-1&rdquo;):</p>
<p><img alt="Early Zero-1 Distributed Optimizer implementation in Megatron-LM." loading="lazy" src="/images/blog_image_2.webp" title="Early Zero-1 Distributed Optimizer implementation in Megatron-LM."></p>
<p>As you can see, the approach is to flatten all optimizer states, concatenate them, and then distribute them evenly across the DP group. This allocation method is optimal for GPU memory because there are no duplicate optimizer states. Moreover, this partitioning is highly beneficial for Muon because most of the local parameters remain complete and can be directly used for the NS operation. Only the parameters at the DP boundaries are split across two DP ranks and become incomplete, requiring special handling.</p>
<p>Specifically, taking DP0 and DP1 jointly processing <code>Param 1</code> as an example, if we were to brainstorm solutions, there are several approaches:</p>
<ol>
<li><strong>The &ldquo;Brainless&rdquo; Gather Method:</strong> DP0 and DP1 each perform a <code>gather</code> to get the full parameters. Both ranks then perform the full NS calculation. After computation, each rank only updates its local portion of the parameters and discards the rest. The <code>grad_reduce_scatter</code> and <code>params_all_gather</code> of steps 1 and 3 remain unchanged to avoid redesigning the algorithm.</li>
<li><strong>Edge Parameter Passing:</strong> Each DP rank <code>i</code> sends its edge parameters to DP <code>i-1</code>. DP <code>i-1</code> is then responsible for the computation on these edge parameters. After calculation, the result is sent back to rank <code>i</code> to update the portion it maintains. This avoids redundant computation, and the communication volume is actually better than the brainless gather method. However, for extreme cases, like a parameter spanning three DP ranks, this requires more complex heuristic arrangements.</li>
<li><strong>Heuristic Precision Arrangement:</strong> When arranging the distributed optimizer, prevent the DP edge-splitting from happening in the first place. This eliminates any extra communication and computation. The cost is that memory allocation is no longer balanced, and finding the optimal allocation becomes a knapsack problem. Unbalanced memory allocation is obviously unacceptable for infrastructure engineers as it leads to inaccurate memory estimation during training, affecting the parallel allocation strategy.</li>
</ol>
<p>In practice, Moonshot uses the <strong>brainless gather method</strong> because it is the simplest to implement and covers all edge cases.  Crucially, the overhead is <strong>small</strong>—only parameters that straddle a DP boundary (≈ <code>DP × 2</code>) incur duplicate computation or extra communication. Other parameters, like <code>param0</code> and <code>param2</code> in the diagram, are complete and don&rsquo;t require any extra work.</p>
<p>Empirically, the actual performance of this communication and computation will be affected by the number of DP ranks and the maximum matrix size in the model. Considering modern MoE architectures (thanks, DeepSeek-V2), a model won&rsquo;t have excessively large matrices because they are all fine-grained experts (and word embedding/lm_head are controlled by AdamW, not Muon). Therefore, in the long run, Muon&rsquo;s scalability has a bright future and is steadily improving.</p>
<p>Since the cost of the brainless method is already low, the benefits of engineering a more complex solution are minimal, which is likely why &ldquo;Jiang Kernel&rdquo; (a nickname for a key person) didn&rsquo;t have the motivation to pursue it further (though I recall You Jiacheng might have implemented some similar solutions on Speedrun?).</p>
<h3 id="23-others-concerns">2.3 Others&rsquo; Concerns<a hidden class="anchor" aria-hidden="true" href="#23-others-concerns">#</a></h3>
<p>However, in the research from some foreign companies, there is a pessimistic bias towards Muon&rsquo;s scalability<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, and Moonshot&rsquo;s method<sup id="fnref2:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> has been repeatedly criticized. Obviously, it&rsquo;s not that everyone else is an idiot. But based on the analysis in 2.2 and the fact that Moonshot successfully trained K2 at a large scale, Moonshot isn&rsquo;t an idiot either.</p>
<p>I personally believe the main reason for this conflict is the <strong>different implementations of Zero-1</strong>, which leads to a large discrepancy in the estimated overhead of Step 2.</p>
<p>The mainstream method abroad is called <strong>dim-0 sharding Zero-1</strong>. For example, the Zero-1 implementation in the mainstream foreign parallel framework, PyTorch FSDP2, is as follows<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>:</p>
<p><img alt="FSDP V2 Sharding on Params Dim 0." loading="lazy" src="/images/blog_image_3.webp" title="FSDP V2 Sharding on Params Dim 0."></p>
<p>And a newer version of Megatron-LM<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> introduced the concept of &ldquo;buckets.&rdquo; The essence of this concept is similar in effect to params dim-0 sharding:</p>
<p><img alt="The new version of Megatron-LM introduces buckets in DDP." loading="lazy" src="/images/blog_image_4.webp" title="The new version of Megatron-LM introduces buckets in DDP."></p>
<p>These updates are actually a &ldquo;devastating&rdquo; blow to the Muon implementation that preceded Moonshot&rsquo;s work. This type of Zero-1 implementation causes <em>every parameter</em> to be sharded by DP! Whether it&rsquo;s the brainless gather method, the edge-passing method, or the sophisticated arrangement method, all of which are based on &ldquo;flat-param concat zero-1,&rdquo; they are all ruined. Every parameter now requires communication and redundant recalculation, leading to a massive amount of extra overhead, making Muon unacceptable.</p>
<h3 id="24-long-term-solution">2.4 Long-Term Solution<a hidden class="anchor" aria-hidden="true" href="#24-long-term-solution">#</a></h3>
<p>Foreign companies are definitely not stupid. Early parallel designs actually all used flat-param concat zero-1<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. Later, due to other concerns (mainly that foreign companies have too many GPUs, and flat params are not conducive to overlapping <code>grad_reduce_scatter</code> and <code>params_all_gather</code>), they switched to dim-0 params sharding Zero-1.</p>
<p>In the context of mandatory dim-0 params sharding, the Moonshot method is indeed not scalable. But this does not mean Muon is inherently unscalable. New solutions will definitely emerge. In fact, I&rsquo;ve heard that it seems possible, and someone might already be working on it (smirking dog face emoji).</p>
<h2 id="3-concern-2-muon-needs-more-hyperparameters">3. Concern 2: Muon Needs More Hyperparameters<a hidden class="anchor" aria-hidden="true" href="#3-concern-2-muon-needs-more-hyperparameters">#</a></h2>
<p>Another common complaint is that Muon has several sets of hyperparameters, which is seen as a significant disadvantage compared to AdamW:</p>
<ol>
<li>It requires additional tuning efforts.</li>
<li>The need for extra tuning means more mental overhead to find the best model, which isn&rsquo;t a fair comparison to AdamW.</li>
<li>If AdamW were also tuned in blocks, it might achieve better results.</li>
</ol>
<p>I personally think this concern stems from a lack of precise understanding of the mathematical properties of the Muon optimizer. To understand Muon, we need to look at it from the perspectives of <strong>Standard Parametrization (SP)</strong> and <strong>Maximal Update Parametrization (µP)</strong> to see why multiple sets of parameters need adjustment.</p>
<p>Additionally, Muon is designed for matrices<sup id="fnref2:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Non-matrix parameters like word embeddings, <code>lm_head</code>, and <code>rmsnorm_gamma</code> are all updated using AdamW.</p>
<h3 id="31-standard-parametrization-sp--muon">3.1 Standard Parametrization (SP) + Muon<a hidden class="anchor" aria-hidden="true" href="#31-standard-parametrization-sp--muon">#</a></h3>
<p>Let&rsquo;s first look at Muon under SP. When Moonshot started researching/reproducing (i.e., copying) Keller&rsquo;s Muon in its early days (around January 2024)<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>, it looked like this (without weight decay and without the various engineering optimizations added by Mr. You, like the zero-1 optimizations):</p>
<p><img alt="Keller Jordan&rsquo;s early version of Muon." loading="lazy" src="/images/blog_image_5.webp" title="Keller Jordan&#39;s early version of Muon."></p>
<p>At this stage, there weren&rsquo;t so many outrageous sets of parameters—just one set for AdamW and one for Muon. However, the update RMS (Root Mean Square) of Muon is very different from that of AdamW. In Moonshot&rsquo;s work<sup id="fnref5:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, Su Yin provided a derivation:</p>
<p><img alt="Su Yin&rsquo;s Update RMS derivation." loading="lazy" src="/images/blog_image_6.webp" title="Su Yin&#39;s Update RMS derivation."></p>
<p>This shows that AdamW&rsquo;s update RMS is empirically around 0.2-0.4, while Muon&rsquo;s is much smaller. If you don&rsquo;t increase Muon&rsquo;s update RMS (the simplest way being a dedicated learning rate), Muon simply won&rsquo;t update effectively, making it an unfair comparison.</p>
<p>In the SP setting, if you don&rsquo;t want to tune two sets of parameters, you can directly use Moonshot&rsquo;s work<sup id="fnref6:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. By matching the update RMS, it&rsquo;s practically &ldquo;out-of-the-box.&rdquo; You can use a single set of AdamW hyperparameters. There&rsquo;s plenty of work on how to tune AdamW hyperparameters (e.g., the <code>stepfun</code> law). Just copy one and migrate it to Muon using Moonshot&rsquo;s method, and you will likely get good improved loss token efficiency.</p>
<p>In fact, the main contribution of Moonshot&rsquo;s work is here: allowing everyone to migrate to Muon in the SP setting without much thought. My superficial understanding is that this is equivalent to the fastest optimization under a matrix Frobenius norm constraint, which effectively controls the update RMS, similar to AdamW. It meets the requirements of SP, but it&rsquo;s not optimal. For Muon, the theoretically optimal method is the fastest optimization under a spectral norm constraint, which we will discuss next.</p>
<h3 id="32-µp-parametrization--muon">3.2 µP Parametrization + Muon<a hidden class="anchor" aria-hidden="true" href="#32-µp-parametrization--muon">#</a></h3>
<p>The most exciting use of Muon is not SP, but its combination with <strong>µP (Maximal Update Parametrization)</strong>. A series of open-source works have provided very exciting introductions! [^16 ]<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup> <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>.</p>
<p>In short, Muon is almost an optimizer tailor-made for µP. Unlike using µP + AdamW, which introduces many variance-based assumptions, Muon naturally controls the <strong>spectral norm</strong> (because NS mathematically clips the max singular values, and the max singular value <em>is</em> the spectral norm by definition). This makes it perfectly suited for the spectral norm control required by high-order µP<sup id="fnref1:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>!</p>
<p>Looking at Keller&rsquo;s improvement history on Muon, besides infrastructure optimizations by masters like Mr. You, the main evolution was the introduction of µP ideas by the &ldquo;god-tier&rdquo; Jeremy Bernstein (Jeremy is an author of both µP and the Muon blog, so he&rsquo;s a double-threat).</p>
<p>After introducing ideas similar to µP, the Embedding, LM Head, and Hidden Matrices all got their own control logic<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>. Although it seems outrageous, it&rsquo;s reasonable when you consider the need to adapt to µP (in fact, adapting AdamW for µP also requires learning rate adjustments for different modules).</p>
<p>In particular, look at the adjustment of Muon&rsquo;s update RMS here. Ignore the <code>max(1, x)</code> part for a moment and just look at the <code>sqrt(d_out/d_in)</code> part. This is <em>exactly</em> the same as the derivation in Su Yin&rsquo;s high-order µP blog<sup id="fnref2:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>! (Though I don&rsquo;t know why the <code>max(1, x)</code> operation was added. With <code>max</code>, it actually reverts to a Frobenius norm-like scaling, doesn&rsquo;t it?)</p>
<p><img alt="Keller&rsquo;s muon update method for adjusting LR on the hidden matrix." loading="lazy" src="/images/blog_image_7.jpg" title="Keller&#39;s muon update method for adjusting LR on the hidden matrix."></p>
<h2 id="4-concern-3-muon-training-instabilities">4. Concern 3: Muon Training Instabilities<a hidden class="anchor" aria-hidden="true" href="#4-concern-3-muon-training-instabilities">#</a></h2>
<p>In reality, few companies train Muon at <em>truly</em> large scale.  Moonshot themselves report only two instability sources<sup id="fnref7:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> <sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>:</p>
<ol>
<li>Weight decay.</li>
<li>The max attention logit problem (addressed by <code>muonclip</code>).</li>
</ol>
<p>Weight decay is easy to understand, while the max attention logit problem involves the <code>muonclip</code> method mentioned in their recent blog<sup id="fnref2:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>.</p>
<p><img alt="Moonshot&rsquo;s self-disclosure and analysis." loading="lazy" src="/images/blog_image_8.webp" title="Moonshot&#39;s self-disclosure and analysis."></p>
<p>The max attention logit problem can usually be solved with <code>qknorm</code>, but Moonshot used MLA (Multi-Head Latent Attention) in K2 (I have to say, DeepSeek is ruthless; their model architectures are tried-and-true winners). The results are probably just that good, so there&rsquo;s no need to force innovation when a great technology already exists. MLA adds normalization during compression, but for inference efficiency, the q and k heads aren&rsquo;t materialized, which means you can&rsquo;t perform qk-head normalization.</p>
<p>Therefore, Moonshot took a different approach and created <code>muonclip</code> (in fact, others have also expressed concerns about the effectiveness of <code>qknorm</code><sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>).</p>
<p><img alt="Moonshot&rsquo;s <code>muonclip</code>." loading="lazy" src="/images/blog_image_9.webp" title="Moonshot&#39;s `muonclip`."></p>
<p>I personally find <code>muonclip</code> very elegant! In Su Yin’s high-order MuP blog<sup id="fnref3:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>, we learn that the spectral norm is smaller than the Frobenius norm:</p>
<p><img alt="The spectral norm is smaller than the Frobenius norm." loading="lazy" src="/images/blog_image_10.webp" title="The spectral norm is smaller than the Frobenius norm."></p>
<p>And the spectral norm is directly tied to the maximum logit size, i.e.</p>
<p><code>||x W||₂ ≤ ||x||₂ · ||W||₂</code></p>
<p>(where <code>W</code> is a matrix, so <code>||W||₂</code> is its spectral norm). The most direct approach is to control the spectral norm. However, the spectral norm is difficult to calculate. So, we can use the inequality relationship between spectral and Frobenius norms and directly clip the Frobenius norm. By doing so, <code>||xW||_2</code> will be controlled!</p>
<p>But later I had a chance to chat with Su Yin, and he said he didn&rsquo;t think that far ahead, and my understanding might not be right (I was floored). His idea was to directly operate on the fundamental problem. Su Yin mentioned he will be releasing a blog post in the next few days, so keep an eye out for that.</p>
<h2 id="5-conclusion">5. Conclusion<a hidden class="anchor" aria-hidden="true" href="#5-conclusion">#</a></h2>
<p>I feel that K2 is going to be a very powerful model, and I look forward to more evaluations from the community. Additionally, Moonshot has been very strong in Vision-Language (VL) and Reinforcement Learning (RL) before, so we can expect that after some more training, a K2-based model for thinking and vision understanding will have a chance to shine!</p>
<p>At the same time, as a company with many masters like Su Yin, &ldquo;Jiang Kernel,&rdquo; and Feilaige&rsquo;s own Zhang Yu, Moonshot feels very promising! Moreover, Moonshot not only implements fancy new technologies like Muon but also generously acknowledges and uses advanced technologies from competitors. I feel that shows great character and vision!</p>
<hr>
<h3 id="footnotes">Footnotes<a hidden class="anchor" aria-hidden="true" href="#footnotes">#</a></h3>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://zhuanlan.zhihu.com/p/1915601328211759191">The Story of Feilai Pavilion (Chinese)</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://kellerjordan.github.io/posts/muon/">Keller Jordan&rsquo;s Muon Blog</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://arxiv.org/abs/2502.16982">Moonshot Muon Paper</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref7:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p><a href="https://kexue.fm/archives/10739">Why Use Muon (Chinese)</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p><a href="https://github.com/NVIDIA/Megatron-LM/pull/1428">Megatron-LM PR for Muon</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p><a href="https://github.com/MoonshotAI/Kimi-K2">Moonshot K2 Announcement</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p><a href="https://github.com/NVIDIA/Megatron-LM/blob/main/docs/source/images/distrib_optimizer/sharding_scheme.png">Megatron-LM Zero-1 Sharding Scheme Image</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p><a href="https://x.com/kellerjordan0/status/1893868235381961140">Keller Jordan defending on X</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p><a href="https://www.essential.ai/blog/infra">Essential AI critiques Moonshot</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p><a href="https://arxiv.org/pdf/2504.05295">Dion&rsquo;s critique of Moonshot</a>&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p><a href="https://x.com/SeunghyunSEO7/status/1943731232119964027">Seunghyun Seo&rsquo;s critique of Moonshot on X</a>&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p><a href="https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md">PyTorch FSDP2 Sharding Docs</a>&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p><a href="https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/distributed/distributed_data_parallel.py#L58">Megatron-LM bucket implementation</a>&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p><a href="https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/_flat_param.py">PyTorch FSDP1 Flat Params</a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p><a href="https://github.com/KellerJordan/Muon/blob/7f9342f50bb418d14a52ec89449e7bc93bebca95/muon.py">Keller&rsquo;s early Muon implementation</a>&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p><a href="https://kexue.fm/archives/10795">High-order µP Derivations (Chinese)</a>&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p><a href="https://x.com/JingyuanLiu123/status/1931223767449309657">Discussion on X about Muon + µP</a>&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p><a href="https://github.com/KellerJordan/Muon/blob/master/muon.py#L157">Keller&rsquo;s latest Muon implementation</a>&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p><a href="https://x.com/giffmana/status/1943731151497027962">Post on X calling <code>qknorm</code> a &ldquo;band-aid&rdquo;</a>&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/translation/">Translation</a></li>
      <li><a href="http://localhost:1313/tags/muon/">Muon</a></li>
      <li><a href="http://localhost:1313/tags/kimi2/">Kimi2</a></li>
      <li><a href="http://localhost:1313/tags/moonshot/">Moonshot</a></li>
      <li><a href="http://localhost:1313/tags/optimizer/">Optimizer</a></li>
      <li><a href="http://localhost:1313/tags/ai/">Ai</a></li>
      <li><a href="http://localhost:1313/tags/technical/">Technical</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="http://localhost:1313/posts/mllms/">
    <span class="title">Next »</span>
    <br>
    <span>MLLMs, VLMs, LVLMs, LMMs...</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Jim Robinson-Bohnslav</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
