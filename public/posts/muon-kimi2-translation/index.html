<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>In Defense of Muon: A Deep Dive into Moonshot&#39;s K2 Optimizer (A Translated Analysis) | Jim Robinson-Bohnslav</title>
<meta name="keywords" content="translation, muon, kimi2, moonshot, technical">
<meta name="description" content="Translation of a detailed technical analysis defending Moonshot AI&#39;s Muon optimizer used in their K2 model">
<meta name="author" content="toothacher17">
<link rel="canonical" href="http://localhost:1313/posts/muon-kimi2-translation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.1d79e034ed3fae3cce466e1b13b697315f9998d1ba8d908def178611b372b49f.css" integrity="sha256-HXngNO0/rjzORm4bE7aXMV&#43;ZmNG6jZCN7xeGEbNytJ8=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/muon-kimi2-translation/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-VWMGL11SYN"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-VWMGL11SYN');
        }
      </script><meta property="og:url" content="http://localhost:1313/posts/muon-kimi2-translation/">
  <meta property="og:site_name" content="Jim Robinson-Bohnslav">
  <meta property="og:title" content="In Defense of Muon: A Deep Dive into Moonshot&#39;s K2 Optimizer (A Translated Analysis)">
  <meta property="og:description" content="Translation of a detailed technical analysis defending Moonshot AI&#39;s Muon optimizer used in their K2 model">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-07-12T14:05:31-04:00">
    <meta property="article:modified_time" content="2025-07-12T14:05:31-04:00">
    <meta property="article:tag" content="Translation">
    <meta property="article:tag" content="Muon">
    <meta property="article:tag" content="Kimi2">
    <meta property="article:tag" content="Moonshot">
    <meta property="article:tag" content="Technical">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="In Defense of Muon: A Deep Dive into Moonshot&#39;s K2 Optimizer (A Translated Analysis)">
<meta name="twitter:description" content="Translation of a detailed technical analysis defending Moonshot AI&#39;s Muon optimizer used in their K2 model">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "In Defense of Muon: A Deep Dive into Moonshot's K2 Optimizer (A Translated Analysis)",
      "item": "http://localhost:1313/posts/muon-kimi2-translation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "In Defense of Muon: A Deep Dive into Moonshot's K2 Optimizer (A Translated Analysis)",
  "name": "In Defense of Muon: A Deep Dive into Moonshot\u0027s K2 Optimizer (A Translated Analysis)",
  "description": "Translation of a detailed technical analysis defending Moonshot AI's Muon optimizer used in their K2 model",
  "keywords": [
    "translation", "muon", "kimi2", "moonshot", "technical"
  ],
  "articleBody": "About the translation This is a translation of the original blog post by toothacher17. The original post is in Chinese and can be found here. The author‚Äôs tweet about it is here. I translated it using Google Translate, Deepseek-R1, Gemini 2.5 Pro, and O3. This translation was edited by Kimi K2-Instruct at kimi.com.\nOriginal Post Author: toothacher17\nOriginal Link: https://www.zhihu.com/question/1927140506573435010/answer/1927378524513219780\nSource: Zhihu\nCopyright belongs to the author. For commercial reprint, please contact the author for authorization. For non-commercial reprint, please indicate the source.\nDisclaimer: Self-invited commentator, ex-Moonshot ‚ÄúNo.1 Jiang Kernel hype-man‚Äù (might be duking it out with @Andrew Lu), Feilai Pavilion1 stan and @ËãèÂâëÊûó disciple - shamelessly riding K2‚Äôs coattails for clout.\n1. Concerns Around Using the Muon Optimizer Moonshot‚Äôs freshly-dropped K2 was trained end-to-end with the Muon optimizer23 ‚Äì no AdamW safety-net. Muon was first proposed by Keller, crushed it on the Speedrun leaderboard2, and then got the Moonshot treatment‚Äîsome tweaks and serious scale-up3 4.\nIn Moonshot‚Äôs early tech report3 they claimed Muon brings god-level token efficiency and even open-sourced a Megatron-LM implementation5. Still, X (Twitter) was quick to throw tomatoes. Three memes keep resurfacing:\n‚ÄúMuon needs the whole parameter matrix for NS ‚Äî RIP PCIe.‚Äù Muon requires the full parameter matrix for its ‚ÄúNormalized Stochasticity‚Äù (NS) calculation. In the parallel setting of modern LLM training infrastructure, many believe operating on the full parameter matrix is too expensive.\n‚ÄúMuon = more hyper-parameter sets = more pain.‚Äù Muon requires ‚Äúseveral sets‚Äù of different hyperparameter tuning mechanisms, which places higher demands on model tuning‚Äîa stark contrast to AdamW‚Äôs ‚Äújust one knob‚Äù simplicity.\n‚ÄúMuon training is a minefield‚Äîlook at that attention logit blow-up!‚Äù Muon might cause training instability. For instance, Moonshot‚Äôs own paper3 mentioned a potential issue with the attention max logit.\nWith K2 now out, these fears seem less scary. This blog tries to quibble (Áã°Ëæ©) about why.\n2. Concern #1 ‚Äì ‚ÄúMuon Doesn‚Äôt Scale‚Äù (TL;DR ‚Äì we‚Äôre going to plug the hole the original Muon paper left about infra cost and prove it scales.)\nFirst, let‚Äôs dig into whether operating on Muon‚Äôs full parameter matrix is truly that expensive, and in doing so, fill a small gap left in the previous paper3.\nTo get to the bottom of this, we‚Äôll dive into Zero-1 sharding. Understanding its implementation‚Äîand the key differences between Chinese and international training clusters‚Äîis the only way to explain why Moonshot thinks Muon scales, while others on X remain skeptical. (And by ‚Äúinternational,‚Äù we mostly mean foreign companies, who are so flush with cash and GPUs they have‚Ä¶ different problems.)\n2.1 Zero-1 Sharding Crash-Course Modern LLM training relies on Zero-1 sharding (implemented in Megatron-LM/DeepSpeed/FSDP).\nZero-1 technology essentially shards the optimizer states‚Äîwhich consume a lot of GPU memory (e.g., AdamW‚Äôs exp, exp_square, fp32_master_weights)‚Äîacross the Data Parallel (DP) group.\nWhen using AdamW, the lifecycle of the Zero-1 Distributed Optimizer is as follows:\nGradient Reduce-Scatter: Perform a reduce_scatter of gradients between DP ranks. It‚Äôs a reduce_scatter instead of an all_reduce because of the sharding. Each DP rank only needs to ensure the gradients for the local parameters it‚Äôs responsible for are accurate. Local Parameter Update: Perform the AdamW update calculation for the local parameters. Since AdamW‚Äôs calculation is element-wise, this step only needs to compute the updates for local parameters. Parameter All-Gather: Perform a parameter all_gather between DP ranks. Because each DP rank only updated a portion of the parameters, an all_gather is needed for all ranks to get the complete, updated set of parameters. (Non-matrix params ‚Äì word embeddings, lm_head, rmsnorm_gamma ‚Äì stay on boring old AdamW.) Note that steps 1 and 3, while seemingly communication-heavy, can actually be overlapped with the model‚Äôs forward/backward pass (a very mature technique all major frameworks implement), so there‚Äôs no need to worry. In step 2, since AdamW is element-wise and the computation per rank decreases as DP size increases, it‚Äôs highly scalable.\nIn summary, Zero-1 makes AdamW so cheap it usually eats \u003c 1 % of global-step wall-clock ‚Äì basically noise.\nHowever, Muon faces a significant challenge in step 2 because its calculation is not element-wise. Muon requires the full parameter matrix to compute NS, which inevitably introduces additional communication and a larger computational load (running NS on the entire matrix).\nFor Muon to be as scalable as possible, the communication overhead of step 2 needs to be minimal (as it can hardly be hidden), and the additional computation introduced needs to be as small as possible (a single small matrix runs NS quickly, so we should avoid running NS on overly large or numerous matrices per DP rank).\n2.2 The Moonshot Solution Based on Moonshot‚Äôs open-source work5, it‚Äôs speculated that their development is based on a version of Megatron-LM that they have since maintained. For Megatron-LM, its early implementation of the Zero-1 optimizer6 is as follows (we‚Äôll call it ‚Äúflat-param concat zero-1‚Äù):\nThe approach is to flatten all optimizer states, concatenate them, and then distribute them evenly across the DP group. This allocation method is memory-optimal (no duplicate states) and highly Muon-friendly, since most local parameters remain complete matrices. Only the parameters at the DP boundaries get split across two ranks, requiring special handling.\nSpecifically, taking DP0 and DP1 jointly processing Param 1 as an example, if we were to brainstorm solutions, there are several approaches:\nThe ‚ÄúBrainless‚Äù Gather Method: DP0 and DP1 each perform a gather to get the full parameters. Both ranks then perform the full NS calculation. After computation, each rank only updates its local portion of the parameters and discards the rest. The grad_reduce_scatter and params_all_gather of steps 1 and 3 remain unchanged to avoid redesigning the algorithm. Edge Parameter Passing: Each DP rank i sends its edge parameters to DP i-1. DP i-1 is then responsible for the computation on these edge parameters. After calculation, the result is sent back to rank i to update the portion it maintains. This avoids redundant computation, and the communication volume is actually better than the brainless gather method. However, for extreme cases, like a parameter spanning three DP ranks, this requires more complex heuristic arrangements. Heuristic Precision Arrangement: When arranging the distributed optimizer, prevent the DP edge-splitting from happening in the first place. This eliminates any extra communication and computation. The cost is that memory allocation is no longer balanced, and finding the optimal allocation becomes a knapsack problem. Unbalanced memory allocation is obviously unacceptable for infrastructure engineers as it leads to inaccurate memory estimation during training, affecting the parallel allocation strategy. In practice, Moonshot uses the brainless gather method because it‚Äôs simple and the overhead is tiny. The whole hack is ~10 LOC ‚Äì infra teams cheer. Only the DP√ó2 edge slices need this treatment; all other parameters are complete and don‚Äôt require any extra work.\nEmpirically, the overhead is negligible because modern MoE architectures (thanks, DeepSeek-V2) don‚Äôt have single, monstrously large matrices. Instead, they use many fine-grained experts (and things like word embedding/lm_head are handled by AdamW, not Muon). Therefore, in the long run, Muon‚Äôs scalability has a bright future.\nSince the cost of the brainless method was already so low, the ROI on engineering a fancier solution was minimal, so ‚ÄúJiang Kernel‚Äù had no motivation to continue optimizing (though I remember You Jiacheng might have implemented some similar hacks on Speedrun?).\n2.3 Others‚Äô Concerns However, in the research from some foreign companies, there is a pessimistic bias towards Muon‚Äôs scalability7 8 9 10, and Moonshot‚Äôs method5 has been repeatedly criticized. Obviously, it‚Äôs not that everyone else is an idiot. But based on the analysis in 2.2 and the fact that Moonshot successfully trained K2 at a large scale, Moonshot isn‚Äôt an idiot either.\nI personally believe the main reason for this conflict is the different implementations of Zero-1, which leads to a large discrepancy in the estimated overhead of Step 2.\nThe mainstream method abroad is called dim-0 sharding Zero-1. For example, the Zero-1 implementation in the mainstream foreign parallel framework, PyTorch FSDP2, is as follows11:\nAnd a newer version of Megatron-LM12 introduced the concept of ‚Äúbuckets.‚Äù The essence of this concept is similar in effect to params dim-0 sharding:\nThese updates are a ‚Äúdevastating‚Äù blow to the Muon implementation that preceded Moonshot‚Äôs work. This type of Zero-1 implementation causes every parameter to be sharded by DP. The methods we discussed, all based on ‚Äúflat-param concat zero-1,‚Äù are completely ruined. Every parameter now requires communication and redundant recalculation, leading to a massive amount of extra overhead ‚Äì Muon is basically DOA under dim-0 sharding.\n2.4 Long-Term Solution Foreign companies are definitely not stupid. Early parallel designs actually all used flat-param concat zero-113. Later, due to other concerns (mainly that foreign companies have too many GPUs, and flat params are not conducive to overlapping grad_reduce_scatter and params_all_gather), they switched to dim-0 params sharding Zero-1.\nIn the context of mandatory dim-0 params sharding, the Moonshot method is indeed not scalable. But this does not mean Muon is inherently unscalable. New solutions will definitely emerge. In fact, I‚Äôve heard that it seems possible, and someone might already be working on it üê∂.\n3. Concern #2 ‚Äì ‚ÄúMuon needs more hyper-parameters‚Äù Another common complaint is that Muon has several sets of hyperparameters, which is seen as a significant disadvantage compared to AdamW:\nIt requires additional tuning efforts. The need for extra tuning means more mental overhead to find the best model, which isn‚Äôt a fair comparison to AdamW. If AdamW were also tuned in blocks, it might achieve better results. I personally think this concern stems from a lack of precise understanding of the mathematical properties of the Muon optimizer. To understand Muon, we need to look at it from the perspectives of Standard Parametrization (SP) and Maximal Update Parametrization (¬µP) to see why multiple sets of parameters need adjustment.\nAdditionally, Muon is designed for matrices2. Non-matrix parameters like word embeddings, lm_head, and rmsnorm_gamma are all updated using AdamW.\n3.1 Standard Parametrization (SP) + Muon Let‚Äôs first look at Muon under SP. When Moonshot started researching/reproducing (copying) Keller‚Äôs Muon in the early period (around December 2024)14, it looked like this (without weight decay and without the various engineering optimizations added by Mr. You, like the zero-1 optimizations):\nAt this stage, there weren‚Äôt so many outrageous sets of parameters‚Äîjust one set for AdamW and one for Muon. However, the update RMS (Root Mean Square) of Muon is very different from that of AdamW. In Moonshot‚Äôs work3, Su Yin provided a derivation:\nThis shows that AdamW‚Äôs update RMS is empirically around 0.2-0.4, while Muon‚Äôs is much smaller. If you don‚Äôt increase Muon‚Äôs update RMS (the simplest way being a dedicated learning rate), Muon simply won‚Äôt update effectively, making it an unfair comparison.\nIn the SP setting, if you don‚Äôt want to tune two sets of parameters, you can directly use Moonshot‚Äôs work3. By matching the update RMS, it‚Äôs practically ‚Äúout-of-the-box.‚Äù You can use a single set of AdamW hyperparameters. There‚Äôs plenty of work on how to tune AdamW hyperparameters (e.g., the stepfun law). Moonshot‚Äôs adapter means you can literally copy-paste any AdamW LR schedule and call it a day. Just copy one and migrate it to Muon using Moonshot‚Äôs method, and you will likely get good improved loss token efficiency.\nIn fact, the main contribution of Moonshot‚Äôs work is here: allowing everyone to migrate to Muon in the SP setting without much thought. My superficial understanding is that this is equivalent to the fastest optimization under a matrix Frobenius norm constraint, which effectively controls the update RMS, similar to AdamW. It meets the requirements of SP, but it‚Äôs not optimal. For Muon, the theoretically optimal method is the fastest optimization under a spectral norm constraint, which we will discuss next.\n3.2 ¬µP Parametrization + Muon The most exciting use of Muon is not SP, but its combination with ¬µP (Maximal Update Parametrization). A series of open-source works have provided very exciting introductions! [^16 ]15 16.\nIn short, Muon is almost an optimizer tailor-made for ¬µP. Unlike using ¬µP + AdamW, which introduces many variance-based assumptions, Muon naturally controls the spectral norm (because NS mathematically clips the max singular values, and the max singular value is the spectral norm by definition). This makes it perfectly suited for the spectral norm control required by high-order ¬µP15!\nLooking at Keller‚Äôs improvement history on Muon, besides infrastructure optimizations by masters like Mr. You, the main evolution was the introduction of ¬µP ideas by Jeremy Bernstein (Jeremy is an author of both ¬µP and the Muon blog, so he‚Äôs double god-tier).\nAfter introducing ideas similar to ¬µP, the Embedding, LM Head, and Hidden Matrices all got their own control logic17. Although it seems outrageous, it‚Äôs reasonable when you consider the need to adapt to ¬µP (in fact, adapting AdamW for ¬µP also requires learning rate adjustments for different modules).\nIn particular, look at the adjustment of Muon‚Äôs update RMS here. Ignore the max(1, x) part for a moment and just look at the sqrt(d_out/d_in) part. This is exactly the same as the derivation in Su Yin‚Äôs high-order ¬µP blog15! (Though I don‚Äôt know why the max(1, x) operation was added. With max, it actually reverts to a Frobenius norm-like scaling, doesn‚Äôt it?)\n4. Concern 3: Muon Training Instabilities In reality, few companies train Muon at truly large scale. Moonshot themselves report only two instability sources3 18:\nWeight decay. The max attention logit problem (addressed by muonclip). Weight decay is easy to understand, while the max attention logit problem involves the muonclip method mentioned in their recent blog18.\nThe max attention logit problem can usually be solved with qknorm, but Moonshot used MLA (Multi-Head Latent Attention) in K2 (I have to say, DeepSeek is ruthless; their model architectures are tried-and-true winners). The results are probably just that good, so there‚Äôs no need to force innovation when a great technology already exists. MLA adds normalization during compression, but for inference efficiency, the q and k heads aren‚Äôt materialized, which means you can‚Äôt perform qk-head normalization.\nTherefore, Moonshot took a different approach and created muonclip (in fact, others have also expressed concerns about the effectiveness of qknorm19).\nI personally find muonclip very elegant! In Su Yin‚Äôs high-order MuP blog15, we learn that the spectral norm is smaller than the Frobenius norm:\nAnd the spectral norm is directly tied to the maximum logit size, i.e.\n||x W||‚ÇÇ ‚â§ ||x||‚ÇÇ ¬∑ ||W||‚ÇÇ\n(where W is a matrix, so ||W||‚ÇÇ is its spectral norm). The most direct approach is to control the spectral norm. However, the spectral norm is difficult to calculate. So, we can use the inequality relationship between spectral and Frobenius norms and directly clip the Frobenius norm. By doing so, ||xW||_2 will be controlled!\nBut later I had a chance to chat with Su Yin, and he said he didn‚Äôt think that far ahead, and my understanding might not be right (‰∫∫È∫ª‰∫Ü). His idea was to directly operate on the fundamental problem. Su Yin mentioned he will be releasing a blog post in the next few days, so keep an eye out for that.\n5. Conclusion K2 is shaping up to be cracked.\nMoonshot already crushes VL + RL; once they stack thinking + vision on K2, expect fireworks.\nWith Su Yin, Jiang-kernel, and Feilai-Pavilion‚Äôs Zhang Yu on the roster, Moonshot‚Äôs ceiling is sky-high.\nA company that ships Muon and happily borrows DeepSeek‚Äôs MLA? That‚Äôs big-dick energy.\nFootnotes The Story of Feilai Pavilion (Chinese)¬†‚Ü©Ô∏é\nKeller Jordan‚Äôs Muon Blog¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é\nMoonshot Muon Paper¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é\nWhy Use Muon (Chinese)¬†‚Ü©Ô∏é\nMegatron-LM PR for Muon¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é\nMegatron-LM Zero-1 Sharding Scheme Image¬†‚Ü©Ô∏é\nKeller Jordan defending on X¬†‚Ü©Ô∏é\nEssential AI critiques Moonshot¬†‚Ü©Ô∏é\nDion‚Äôs critique of Moonshot¬†‚Ü©Ô∏é\nSeunghyun Seo‚Äôs critique of Moonshot on X¬†‚Ü©Ô∏é\nPyTorch FSDP2 Sharding Docs¬†‚Ü©Ô∏é\nMegatron-LM bucket implementation¬†‚Ü©Ô∏é\nPyTorch FSDP1 Flat Params¬†‚Ü©Ô∏é\nKeller‚Äôs early Muon implementation¬†‚Ü©Ô∏é\nHigh-order ¬µP Derivations (Chinese)¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é\nDiscussion on X about Muon + ¬µP¬†‚Ü©Ô∏é\nKeller‚Äôs latest Muon implementation¬†‚Ü©Ô∏é\nMoonshot K2 Announcement¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é\nPost on X calling qknorm a ‚Äúband-aid‚Äù¬†‚Ü©Ô∏é\n",
  "wordCount" : "2633",
  "inLanguage": "en",
  "datePublished": "2025-07-12T14:05:31-04:00",
  "dateModified": "2025-07-12T14:05:31-04:00",
  "author":{
    "@type": "Person",
    "name": "toothacher17"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/muon-kimi2-translation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jim Robinson-Bohnslav",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Jim Robinson-Bohnslav (Alt + H)">Jim Robinson-Bohnslav</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/about/" title="about">
                    <span>about</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="archives">
                    <span>archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;¬ª&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      In Defense of Muon: A Deep Dive into Moonshot&#39;s K2 Optimizer (A Translated Analysis)
    </h1>
    <div class="post-description">
      Translation of a detailed technical analysis defending Moonshot AI&#39;s Muon optimizer used in their K2 model
    </div>
    <div class="post-meta"><span title='2025-07-12 14:05:31 -0400 EDT'>July 12, 2025</span>&nbsp;¬∑&nbsp;toothacher17

</div>
  </header> 
  <div class="post-content"><h2 id="about-the-translation">About the translation<a hidden class="anchor" aria-hidden="true" href="#about-the-translation">#</a></h2>
<p>This is a translation of the original blog post by toothacher17. The original post is in Chinese and can be found <a href="https://www.zhihu.com/question/1927140506573435010/answer/1927378524513219780">here</a>. The author&rsquo;s tweet about it is <a href="https://x.com/JingyuanLiu123/status/1944071538569097352">here</a>. I translated it using Google Translate, Deepseek-R1, Gemini 2.5 Pro, and O3. This translation was edited by Kimi K2-Instruct at <a href="https://kimi.com">kimi.com</a>.</p>
<h2 id="original-post">Original Post<a hidden class="anchor" aria-hidden="true" href="#original-post">#</a></h2>
<p><strong>Author:</strong> toothacher17</p>
<p><strong>Original Link:</strong> <a href="https://www.zhihu.com/question/1927140506573435010/answer/1927378524513219780">https://www.zhihu.com/question/1927140506573435010/answer/1927378524513219780</a></p>
<p><strong>Source:</strong> Zhihu</p>
<p><em>Copyright belongs to the author. For commercial reprint, please contact the author for authorization. For non-commercial reprint, please indicate the source.</em></p>
<p><em>Disclaimer: Self-invited commentator, ex-Moonshot &ldquo;No.1 Jiang Kernel hype-man&rdquo; (might be duking it out with @Andrew Lu), Feilai Pavilion<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> stan and @ËãèÂâëÊûó disciple - shamelessly riding K2&rsquo;s coattails for clout.</em></p>
<h2 id="1-concerns-around-using-the-muon-optimizer">1. Concerns Around Using the Muon Optimizer<a hidden class="anchor" aria-hidden="true" href="#1-concerns-around-using-the-muon-optimizer">#</a></h2>
<p>Moonshot&rsquo;s freshly-dropped <strong>K2</strong> was trained <em>end-to-end</em> with the Muon optimizer<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> ‚Äì no AdamW safety-net. Muon was first proposed by Keller, crushed it on the Speedrun leaderboard<sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, and then got the Moonshot treatment‚Äîsome tweaks and serious scale-up<sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p><img alt="Muon in the Moonshot paper has huge advantages." loading="lazy" src="/images/blog_image_1.webp" title="Moonshot&#39;s paper highlighting the significant advantages of Muon."></p>
<p>In Moonshot&rsquo;s early tech report<sup id="fnref2:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> they claimed Muon brings god-level token efficiency and even open-sourced a Megatron-LM implementation<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Still, X (Twitter) was quick to throw tomatoes. Three memes keep resurfacing:</p>
<ol>
<li>
<p><strong>&ldquo;Muon needs the <em>whole</em> parameter matrix for NS ‚Äî RIP PCIe.&rdquo;</strong> Muon requires the full parameter matrix for its &ldquo;Normalized Stochasticity&rdquo; (NS) calculation. In the parallel setting of modern LLM training infrastructure, many believe operating on the full parameter matrix is too expensive.</p>
</li>
<li>
<p><strong>&ldquo;Muon = more hyper-parameter sets = more pain.&rdquo;</strong> Muon requires &ldquo;several sets&rdquo; of different hyperparameter tuning mechanisms, which places higher demands on model tuning‚Äîa stark contrast to AdamW&rsquo;s &ldquo;just one knob&rdquo; simplicity.</p>
</li>
<li>
<p><strong>&ldquo;Muon training is a minefield‚Äîlook at that attention logit blow-up!&rdquo;</strong> Muon might cause training instability. For instance, Moonshot&rsquo;s own paper<sup id="fnref3:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> mentioned a potential issue with the attention max logit.</p>
</li>
</ol>
<p>With K2 now out, these fears seem less scary. This blog tries to <strong>quibble</strong> (Áã°Ëæ©) about why.</p>
<h2 id="2-concern-1--muon-doesnt-scale">2. Concern #1 ‚Äì &ldquo;Muon Doesn&rsquo;t Scale&rdquo;<a hidden class="anchor" aria-hidden="true" href="#2-concern-1--muon-doesnt-scale">#</a></h2>
<p><em>(TL;DR ‚Äì we&rsquo;re going to plug the hole the original Muon paper left about infra cost and prove it scales.)</em></p>
<p>First, let&rsquo;s dig into whether operating on Muon&rsquo;s full parameter matrix is truly that expensive, and in doing so, fill a small gap left in the previous paper<sup id="fnref4:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p>To get to the bottom of this, we&rsquo;ll dive into Zero-1 sharding. Understanding its implementation‚Äîand the key differences between Chinese and international training clusters‚Äîis the only way to explain why Moonshot thinks Muon scales, while others on X remain skeptical. (And by &ldquo;international,&rdquo; we mostly mean foreign companies, who are so flush with cash and GPUs they have&hellip; different problems.)</p>
<h3 id="21-zero-1-sharding-crash-course">2.1 Zero-1 Sharding Crash-Course<a hidden class="anchor" aria-hidden="true" href="#21-zero-1-sharding-crash-course">#</a></h3>
<p>Modern LLM training relies on <strong>Zero-1 sharding</strong> (implemented in Megatron-LM/DeepSpeed/FSDP).</p>
<p>Zero-1 technology essentially shards the optimizer states‚Äîwhich consume a lot of GPU memory (e.g., AdamW&rsquo;s <code>exp</code>, <code>exp_square</code>, <code>fp32_master_weights</code>)‚Äîacross the Data Parallel (DP) group.</p>
<p>When using AdamW, the lifecycle of the Zero-1 Distributed Optimizer is as follows:</p>
<ol>
<li><strong>Gradient Reduce-Scatter:</strong> Perform a <code>reduce_scatter</code> of gradients between DP ranks. It&rsquo;s a <code>reduce_scatter</code> instead of an <code>all_reduce</code> because of the sharding. Each DP rank only needs to ensure the gradients for the local parameters it&rsquo;s responsible for are accurate.</li>
<li><strong>Local Parameter Update:</strong> Perform the AdamW update calculation for the local parameters. Since AdamW&rsquo;s calculation is element-wise, this step only needs to compute the updates for local parameters.</li>
<li><strong>Parameter All-Gather:</strong> Perform a parameter <code>all_gather</code> between DP ranks. Because each DP rank only updated a portion of the parameters, an <code>all_gather</code> is needed for all ranks to get the complete, updated set of parameters.
(Non-matrix params ‚Äì word embeddings, <code>lm_head</code>, <code>rmsnorm_gamma</code> ‚Äì stay on boring old AdamW.)</li>
</ol>
<p>Note that steps 1 and 3, while seemingly communication-heavy, can actually be overlapped with the model&rsquo;s forward/backward pass (a very mature technique all major frameworks implement), so there&rsquo;s no need to worry. In step 2, since AdamW is element-wise and the computation per rank decreases as DP size increases, it&rsquo;s highly scalable.</p>
<p>In summary, Zero-1 makes AdamW so cheap it usually eats <strong>&lt; 1 % of global-step wall-clock</strong> ‚Äì basically noise.</p>
<p>However, Muon faces a significant challenge in step 2 because its calculation is <em>not</em> element-wise. Muon requires the <strong>full parameter matrix</strong> to compute NS, which inevitably introduces additional communication and a larger computational load (running NS on the entire matrix).</p>
<p>For Muon to be as scalable as possible, the communication overhead of step 2 needs to be minimal (as it can hardly be hidden), and the additional computation introduced needs to be as small as possible (a single small matrix runs NS quickly, so we should avoid running NS on overly large or numerous matrices per DP rank).</p>
<h3 id="22-the-moonshot-solution">2.2 The Moonshot Solution<a hidden class="anchor" aria-hidden="true" href="#22-the-moonshot-solution">#</a></h3>
<p>Based on Moonshot&rsquo;s open-source work<sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>, it&rsquo;s speculated that their development is based on a version of Megatron-LM that they have since maintained. For Megatron-LM, its early implementation of the Zero-1 optimizer<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> is as follows (we&rsquo;ll call it &ldquo;flat-param concat zero-1&rdquo;):</p>
<p><img alt="Early Zero-1 Distributed Optimizer implementation in Megatron-LM." loading="lazy" src="/images/blog_image_2.webp" title="Early Zero-1 Distributed Optimizer implementation in Megatron-LM."></p>
<p>The approach is to flatten all optimizer states, concatenate them, and then distribute them evenly across the DP group. This allocation method is memory-optimal (no duplicate states) and highly Muon-friendly, since most local parameters remain complete matrices. Only the parameters at the DP boundaries get split across two ranks, requiring special handling.</p>
<p>Specifically, taking DP0 and DP1 jointly processing <code>Param 1</code> as an example, if we were to brainstorm solutions, there are several approaches:</p>
<ol>
<li><strong>The &ldquo;Brainless&rdquo; Gather Method:</strong> DP0 and DP1 each perform a <code>gather</code> to get the full parameters. Both ranks then perform the full NS calculation. After computation, each rank only updates its local portion of the parameters and discards the rest. The <code>grad_reduce_scatter</code> and <code>params_all_gather</code> of steps 1 and 3 remain unchanged to avoid redesigning the algorithm.</li>
<li><strong>Edge Parameter Passing:</strong> Each DP rank <code>i</code> sends its edge parameters to DP <code>i-1</code>. DP <code>i-1</code> is then responsible for the computation on these edge parameters. After calculation, the result is sent back to rank <code>i</code> to update the portion it maintains. This avoids redundant computation, and the communication volume is actually better than the brainless gather method. However, for extreme cases, like a parameter spanning three DP ranks, this requires more complex heuristic arrangements.</li>
<li><strong>Heuristic Precision Arrangement:</strong> When arranging the distributed optimizer, prevent the DP edge-splitting from happening in the first place. This eliminates any extra communication and computation. The cost is that memory allocation is no longer balanced, and finding the optimal allocation becomes a knapsack problem. Unbalanced memory allocation is obviously unacceptable for infrastructure engineers as it leads to inaccurate memory estimation during training, affecting the parallel allocation strategy.</li>
</ol>
<p>In practice, Moonshot uses the <strong>brainless gather method</strong> because it&rsquo;s simple and the overhead is tiny. The whole hack is ~10 LOC ‚Äì infra teams cheer. Only the <code>DP√ó2</code> edge slices need this treatment; all other parameters are complete and don&rsquo;t require any extra work.</p>
<p>Empirically, the overhead is negligible because modern MoE architectures (thanks, DeepSeek-V2) don&rsquo;t have single, monstrously large matrices. Instead, they use many fine-grained experts (and things like word embedding/lm_head are handled by AdamW, not Muon). Therefore, in the long run, Muon&rsquo;s scalability has a bright future.</p>
<p>Since the cost of the brainless method was already so low, the ROI on engineering a fancier solution was minimal, so &ldquo;Jiang Kernel&rdquo; had no motivation to continue optimizing (though I remember You Jiacheng might have implemented some similar hacks on Speedrun?).</p>
<h3 id="23-others-concerns">2.3 Others&rsquo; Concerns<a hidden class="anchor" aria-hidden="true" href="#23-others-concerns">#</a></h3>
<p>However, in the research from some foreign companies, there is a pessimistic bias towards Muon&rsquo;s scalability<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>, and Moonshot&rsquo;s method<sup id="fnref2:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> has been repeatedly criticized. Obviously, it&rsquo;s not that everyone else is an idiot. But based on the analysis in 2.2 and the fact that Moonshot successfully trained K2 at a large scale, Moonshot isn&rsquo;t an idiot either.</p>
<p>I personally believe the main reason for this conflict is the <strong>different implementations of Zero-1</strong>, which leads to a large discrepancy in the estimated overhead of Step 2.</p>
<p>The mainstream method abroad is called <strong>dim-0 sharding Zero-1</strong>. For example, the Zero-1 implementation in the mainstream foreign parallel framework, PyTorch FSDP2, is as follows<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>:</p>
<p><img alt="FSDP V2 Sharding on Params Dim 0." loading="lazy" src="/images/blog_image_3.webp" title="FSDP V2 Sharding on Params Dim 0."></p>
<p>And a newer version of Megatron-LM<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> introduced the concept of &ldquo;buckets.&rdquo; The essence of this concept is similar in effect to params dim-0 sharding:</p>
<p><img alt="The new version of Megatron-LM introduces buckets in DDP." loading="lazy" src="/images/blog_image_4.webp" title="The new version of Megatron-LM introduces buckets in DDP."></p>
<p>These updates are a &ldquo;devastating&rdquo; blow to the Muon implementation that preceded Moonshot&rsquo;s work. This type of Zero-1 implementation causes <em>every parameter</em> to be sharded by DP. The methods we discussed, all based on &ldquo;flat-param concat zero-1,&rdquo; are completely ruined. Every parameter now requires communication and redundant recalculation, leading to a massive amount of extra overhead ‚Äì Muon is basically <strong>DOA</strong> under dim-0 sharding.</p>
<h3 id="24-long-term-solution">2.4 Long-Term Solution<a hidden class="anchor" aria-hidden="true" href="#24-long-term-solution">#</a></h3>
<p>Foreign companies are definitely not stupid. Early parallel designs actually all used flat-param concat zero-1<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>. Later, due to other concerns (mainly that foreign companies have too many GPUs, and flat params are not conducive to overlapping <code>grad_reduce_scatter</code> and <code>params_all_gather</code>), they switched to dim-0 params sharding Zero-1.</p>
<p>In the context of mandatory dim-0 params sharding, the Moonshot method is indeed not scalable. But this does not mean Muon is inherently unscalable. New solutions will definitely emerge. In fact, I&rsquo;ve heard that it seems possible, and someone might already be working on it üê∂.</p>
<h2 id="3-concern-2--muon-needs-more-hyper-parameters">3. Concern #2 ‚Äì &ldquo;Muon needs more hyper-parameters&rdquo;<a hidden class="anchor" aria-hidden="true" href="#3-concern-2--muon-needs-more-hyper-parameters">#</a></h2>
<p>Another common complaint is that Muon has several sets of hyperparameters, which is seen as a significant disadvantage compared to AdamW:</p>
<ol>
<li>It requires additional tuning efforts.</li>
<li>The need for extra tuning means more mental overhead to find the best model, which isn&rsquo;t a fair comparison to AdamW.</li>
<li>If AdamW were also tuned in blocks, it might achieve better results.</li>
</ol>
<p>I personally think this concern stems from a lack of precise understanding of the mathematical properties of the Muon optimizer. To understand Muon, we need to look at it from the perspectives of <strong>Standard Parametrization (SP)</strong> and <strong>Maximal Update Parametrization (¬µP)</strong> to see why multiple sets of parameters need adjustment.</p>
<p>Additionally, Muon is designed for matrices<sup id="fnref2:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Non-matrix parameters like word embeddings, <code>lm_head</code>, and <code>rmsnorm_gamma</code> are all updated using AdamW.</p>
<h3 id="31-standard-parametrization-sp--muon">3.1 Standard Parametrization (SP) + Muon<a hidden class="anchor" aria-hidden="true" href="#31-standard-parametrization-sp--muon">#</a></h3>
<p>Let&rsquo;s first look at Muon under SP. When Moonshot started researching/reproducing (copying) Keller&rsquo;s Muon in the early period (around December 2024)<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>, it looked like this (without weight decay and without the various engineering optimizations added by Mr. You, like the zero-1 optimizations):</p>
<p><img alt="Keller Jordan&rsquo;s early version of Muon." loading="lazy" src="/images/blog_image_5.webp" title="Keller Jordan&#39;s early version of Muon."></p>
<p>At this stage, there weren&rsquo;t so many outrageous sets of parameters‚Äîjust one set for AdamW and one for Muon. However, the update RMS (Root Mean Square) of Muon is very different from that of AdamW. In Moonshot&rsquo;s work<sup id="fnref5:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, Su Yin provided a derivation:</p>
<p><img alt="Su Yin&rsquo;s Update RMS derivation." loading="lazy" src="/images/blog_image_6.webp" title="Su Yin&#39;s Update RMS derivation."></p>
<p>This shows that AdamW&rsquo;s update RMS is empirically around 0.2-0.4, while Muon&rsquo;s is much smaller. If you don&rsquo;t increase Muon&rsquo;s update RMS (the simplest way being a dedicated learning rate), Muon simply won&rsquo;t update effectively, making it an unfair comparison.</p>
<p>In the SP setting, if you don&rsquo;t want to tune two sets of parameters, you can directly use Moonshot&rsquo;s work<sup id="fnref6:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. By matching the update RMS, it&rsquo;s practically &ldquo;out-of-the-box.&rdquo; You can use a single set of AdamW hyperparameters. There&rsquo;s plenty of work on how to tune AdamW hyperparameters (e.g., the <code>stepfun</code> law). Moonshot&rsquo;s adapter means you can literally <strong>copy-paste</strong> any AdamW LR schedule and call it a day. Just copy one and migrate it to Muon using Moonshot&rsquo;s method, and you will likely get good improved loss token efficiency.</p>
<p>In fact, the main contribution of Moonshot&rsquo;s work is here: allowing everyone to migrate to Muon in the SP setting without much thought. My superficial understanding is that this is equivalent to the fastest optimization under a matrix Frobenius norm constraint, which effectively controls the update RMS, similar to AdamW. It meets the requirements of SP, but it&rsquo;s not optimal. For Muon, the theoretically optimal method is the fastest optimization under a spectral norm constraint, which we will discuss next.</p>
<h3 id="32-¬µp-parametrization--muon">3.2 ¬µP Parametrization + Muon<a hidden class="anchor" aria-hidden="true" href="#32-¬µp-parametrization--muon">#</a></h3>
<p>The most exciting use of Muon is not SP, but its combination with <strong>¬µP (Maximal Update Parametrization)</strong>. A series of open-source works have provided very exciting introductions! [^16 ]<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>.</p>
<p>In short, Muon is almost an optimizer tailor-made for ¬µP. Unlike using ¬µP + AdamW, which introduces many variance-based assumptions, Muon naturally controls the <strong>spectral norm</strong> (because NS mathematically clips the max singular values, and the max singular value <em>is</em> the spectral norm by definition). This makes it perfectly suited for the spectral norm control required by high-order ¬µP<sup id="fnref1:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>!</p>
<p>Looking at Keller&rsquo;s improvement history on Muon, besides infrastructure optimizations by masters like Mr. You, the main evolution was the introduction of ¬µP ideas by Jeremy Bernstein (Jeremy is an author of both ¬µP and the Muon blog, so he&rsquo;s double god-tier).</p>
<p>After introducing ideas similar to ¬µP, the Embedding, LM Head, and Hidden Matrices all got their own control logic<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>. Although it seems outrageous, it&rsquo;s reasonable when you consider the need to adapt to ¬µP (in fact, adapting AdamW for ¬µP also requires learning rate adjustments for different modules).</p>
<p>In particular, look at the adjustment of Muon&rsquo;s update RMS here. Ignore the <code>max(1, x)</code> part for a moment and just look at the <code>sqrt(d_out/d_in)</code> part. This is <em>exactly</em> the same as the derivation in Su Yin&rsquo;s high-order ¬µP blog<sup id="fnref2:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>! (Though I don&rsquo;t know why the <code>max(1, x)</code> operation was added. With <code>max</code>, it actually reverts to a Frobenius norm-like scaling, doesn&rsquo;t it?)</p>
<p><img alt="Keller&rsquo;s muon update method for adjusting LR on the hidden matrix." loading="lazy" src="/images/blog_image_7.jpg" title="Keller&#39;s muon update method for adjusting LR on the hidden matrix."></p>
<h2 id="4-concern-3-muon-training-instabilities">4. Concern 3: Muon Training Instabilities<a hidden class="anchor" aria-hidden="true" href="#4-concern-3-muon-training-instabilities">#</a></h2>
<p>In reality, few companies train Muon at <em>truly</em> large scale.  Moonshot themselves report only two instability sources<sup id="fnref7:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> <sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>:</p>
<ol>
<li>Weight decay.</li>
<li>The max attention logit problem (addressed by <code>muonclip</code>).</li>
</ol>
<p>Weight decay is easy to understand, while the max attention logit problem involves the <code>muonclip</code> method mentioned in their recent blog<sup id="fnref1:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>.</p>
<p><img alt="Moonshot&rsquo;s self-disclosure and analysis." loading="lazy" src="/images/blog_image_8.webp" title="Moonshot&#39;s self-disclosure and analysis."></p>
<p>The max attention logit problem can usually be solved with <code>qknorm</code>, but Moonshot used MLA (Multi-Head Latent Attention) in K2 (I have to say, DeepSeek is ruthless; their model architectures are tried-and-true winners). The results are probably just that good, so there&rsquo;s no need to force innovation when a great technology already exists. MLA adds normalization during compression, but for inference efficiency, the q and k heads aren&rsquo;t materialized, which means you can&rsquo;t perform qk-head normalization.</p>
<p>Therefore, Moonshot took a different approach and created <code>muonclip</code> (in fact, others have also expressed concerns about the effectiveness of <code>qknorm</code><sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>).</p>
<p><img alt="Moonshot&rsquo;s <code>muonclip</code>." loading="lazy" src="/images/blog_image_9.webp" title="Moonshot&#39;s `muonclip`."></p>
<p>I personally find <code>muonclip</code> very elegant! In Su Yin‚Äôs high-order MuP blog<sup id="fnref3:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>, we learn that the spectral norm is smaller than the Frobenius norm:</p>
<p><img alt="The spectral norm is smaller than the Frobenius norm." loading="lazy" src="/images/blog_image_10.webp" title="The spectral norm is smaller than the Frobenius norm."></p>
<p>And the spectral norm is directly tied to the maximum logit size, i.e.</p>
<p><code>||x W||‚ÇÇ ‚â§ ||x||‚ÇÇ ¬∑ ||W||‚ÇÇ</code></p>
<p>(where <code>W</code> is a matrix, so <code>||W||‚ÇÇ</code> is its spectral norm). The most direct approach is to control the spectral norm. However, the spectral norm is difficult to calculate. So, we can use the inequality relationship between spectral and Frobenius norms and directly clip the Frobenius norm. By doing so, <code>||xW||_2</code> will be controlled!</p>
<p>But later I had a chance to chat with Su Yin, and he said he didn&rsquo;t think that far ahead, and my understanding might not be right (‰∫∫È∫ª‰∫Ü). His idea was to directly operate on the fundamental problem. Su Yin mentioned he will be releasing a blog post in the next few days, so keep an eye out for that.</p>
<h2 id="5-conclusion">5. Conclusion<a hidden class="anchor" aria-hidden="true" href="#5-conclusion">#</a></h2>
<p>K2 is shaping up to be <strong>cracked</strong>.<br>
Moonshot already crushes VL + RL; once they stack <strong>thinking + vision</strong> on K2, expect fireworks.</p>
<p>With Su Yin, Jiang-kernel, and Feilai-Pavilion‚Äôs Zhang Yu on the roster, Moonshot‚Äôs ceiling is sky-high.<br>
A company that ships Muon <strong>and</strong> happily borrows DeepSeek‚Äôs MLA? That‚Äôs big-dick energy.</p>
<hr>
<h3 id="footnotes">Footnotes<a hidden class="anchor" aria-hidden="true" href="#footnotes">#</a></h3>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://zhuanlan.zhihu.com/p/1915601328211759191">The Story of Feilai Pavilion (Chinese)</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://kellerjordan.github.io/posts/muon/">Keller Jordan&rsquo;s Muon Blog</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://arxiv.org/abs/2502.16982">Moonshot Muon Paper</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref7:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p><a href="https://kexue.fm/archives/10739">Why Use Muon (Chinese)</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p><a href="https://github.com/NVIDIA/Megatron-LM/pull/1428">Megatron-LM PR for Muon</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p><a href="https://github.com/NVIDIA/Megatron-LM/blob/main/docs/source/images/distrib_optimizer/sharding_scheme.png">Megatron-LM Zero-1 Sharding Scheme Image</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p><a href="https://x.com/kellerjordan0/status/1893868235381961140">Keller Jordan defending on X</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p><a href="https://www.essential.ai/blog/infra">Essential AI critiques Moonshot</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p><a href="https://arxiv.org/pdf/2504.05295">Dion&rsquo;s critique of Moonshot</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p><a href="https://x.com/SeunghyunSEO7/status/1943731232119964027">Seunghyun Seo&rsquo;s critique of Moonshot on X</a>&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p><a href="https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md">PyTorch FSDP2 Sharding Docs</a>&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p><a href="https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/distributed/distributed_data_parallel.py#L58">Megatron-LM bucket implementation</a>&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p><a href="https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/_flat_param.py">PyTorch FSDP1 Flat Params</a>&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p><a href="https://github.com/KellerJordan/Muon/blob/7f9342f50bb418d14a52ec89449e7bc93bebca95/muon.py">Keller&rsquo;s early Muon implementation</a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p><a href="https://kexue.fm/archives/10795">High-order ¬µP Derivations (Chinese)</a>&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p><a href="https://x.com/JingyuanLiu123/status/1931223767449309657">Discussion on X about Muon + ¬µP</a>&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p><a href="https://github.com/KellerJordan/Muon/blob/master/muon.py#L157">Keller&rsquo;s latest Muon implementation</a>&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p><a href="https://github.com/MoonshotAI/Kimi-K2">Moonshot K2 Announcement</a>&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p><a href="https://x.com/giffmana/status/1943731151497027962">Post on X calling <code>qknorm</code> a &ldquo;band-aid&rdquo;</a>&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/translation/">Translation</a></li>
      <li><a href="http://localhost:1313/tags/muon/">Muon</a></li>
      <li><a href="http://localhost:1313/tags/kimi2/">Kimi2</a></li>
      <li><a href="http://localhost:1313/tags/moonshot/">Moonshot</a></li>
      <li><a href="http://localhost:1313/tags/technical/">Technical</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="http://localhost:1313/posts/mllms/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>MLLMs, VLMs, LVLMs, LMMs...</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Jim Robinson-Bohnslav</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
