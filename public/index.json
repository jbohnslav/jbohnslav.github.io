[{"content":"There exists a class of models whose inputs are text prompts + images or video. Their outputs are text.\nExample: \u0026ldquo;Explain the joke in this tweet. Be concise.\u0026rdquo; Answer, courtesy of GPT4o:\nThe joke humorously compares \u0026ldquo;the talk\u0026rdquo; about sensitive topics with explaining to kids why there\u0026rsquo;s a server at home. The mock children\u0026rsquo;s book title exaggerates the idea, poking fun at tech enthusiasts whose home servers are significant enough to require a formal explanation to their kids.\nBuilding these models is one of the biggest fields of both industrial AI and academic computer vision research. But no one can agree on what to call them! For the rest of this post I\u0026rsquo;m going to keep calling them \u0026ldquo;these models\u0026rdquo; because I don\u0026rsquo;t know what else to do.\nChoices Multimodal Large Language Models (MLLM)\nThis is probably the most common name for \u0026ldquo;these models\u0026rdquo;. Points in favor: they all use LLMs as a core component. These models generate text, just like LLMs. They are multi-modal, able to process multiple types of inputs.\nThe problem is that this name is not specific enough: imagine a model where you submit an audio file to and ask a question, e.g. \u0026ldquo;What sound is this?\u0026rdquo; + sound.wav -\u0026gt; \u0026ldquo;This is a siren of an emergency vehicle.\u0026rdquo; Such a model would also be an MLLM.\nProponents: vLLM, Llama 31, Pixtral2, Gemini3, Waymo4, Cambrian-15, InternVL2.5 6, Mammoth-VL7, Florence-VL 8, Fei Fei Li (NeurIPS)\nVision-Language Model (VLM)\nVLM is more specific than MLLM, which is good. However, models like CLIP and SigLIP are Vision-Language Models too. They have image encoders, text encoders, can be prompted, etc. But CLIP et al. are not generative; they do not produce text. That makes this term confusing to me.\nProponents: Molmo9, Huggingface (SmolVLM), PaliGemma 2 10, CogVLM11, NVILA12\nLarge Vision-Language Model (LVLM)\n\u0026ldquo;These models\u0026rdquo; are large, use vision, and generate language. Pretty good. But InternViT-6B is a CLIP-style model with 6 billion parameters: it\u0026rsquo;s large by any measure. InternViT isn\u0026rsquo;t generative, so it\u0026rsquo;s not the kind of model I mean. This paper13 even calls CLIP a VLM and \u0026ldquo;these models\u0026rdquo; LVLMs, so I guess generating text is what makes it \u0026ldquo;Large\u0026rdquo;?\nProponents: Qwen2-VL14\nLarge Multimodal Model (LMM)\nThis one is popular with the Llava folks. They get extra credit because their paper defined the field, but I see this as just a variant of MLLM.\nProponents: Llava15, Llava-OneVision16\nMy take I\u0026rsquo;ve been a proponent of calling \u0026ldquo;these models\u0026rdquo; MLLMs. However, models like GPT4o and Gemini Flash 2.0 can consume text, images, video, or audio, and generate text, images, or audio as well. That is truly multimodal. It\u0026rsquo;s such a big difference that the GPT4o system card17 calls it an \u0026ldquo;omni model\u0026rdquo;.\nModels focusing on images and videos specifically are going to be extremely valuable in many domains: robotics, web agents, as components in coding assistants, and in consumer apps. It therefore makes sense to define them as a class distinctly from the \u0026ldquo;omni models.\u0026rdquo;\nThrough writing this post, I\u0026rsquo;ve convinced myself that VLM is a more specific, useful term. With great apologies to Lucas Beyer and the rest of the SigLIP team, I will call models that learn a joint embedding space between images and text \u0026ldquo;CLIP-style models.\u0026rdquo;\nWhat do you think we should call VLMs? Let\u0026rsquo;s discuss on Twitter or BlueSky.\nReferences A. Dubey et al., \u0026ldquo;The Llama 3 Herd of Models,\u0026rdquo; Jul. 31, 2024, arXiv: arXiv:2407.21783. Accessed: Aug. 01, 2024. [Online]. Available: http://arxiv.org/abs/2407.21783\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agrawal et al., \u0026ldquo;Pixtral 12B,\u0026rdquo; Oct. 09, 2024, arXiv: arXiv:2410.07073. Accessed: Oct. 10, 2024. [Online]. Available: http://arxiv.org/abs/2410.07073\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Team et al., \u0026ldquo;Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,\u0026rdquo; Aug. 08, 2024, arXiv: arXiv:2403.05530. doi: 10.48550/arXiv.2403.05530.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ.-J. Hwang et al., \u0026ldquo;EMMA: End-to-End Multimodal Model for Autonomous Driving,\u0026rdquo; Oct. 30, 2024, arXiv: arXiv:2410.23262. Accessed: Nov. 04, 2024. [Online]. Available: http://arxiv.org/abs/2410.23262\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Tong et al., \u0026ldquo;Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs,\u0026rdquo; Jun. 24, 2024, arXiv: arXiv:2406.16860. Accessed: Jun. 25, 2024. [Online]. Available: http://arxiv.org/abs/2406.16860\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Chen et al., \u0026ldquo;Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling,\u0026rdquo; Dec. 06, 2024, arXiv: arXiv:2412.05271. doi: 10.48550/arXiv.2412.05271.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Guo et al., \u0026ldquo;MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale,\u0026rdquo; Dec. 06, 2024, arXiv: arXiv:2412.05237. doi: 10.48550/arXiv.2412.05237.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Chen et al., \u0026ldquo;Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion,\u0026rdquo; Dec. 05, 2024, arXiv: arXiv:2412.04424. doi: 10.48550/arXiv.2412.04424.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Deitke et al., \u0026ldquo;Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models,\u0026rdquo; Sep. 25, 2024, arXiv: arXiv:2409.17146. Accessed: Sep. 26, 2024. [Online]. Available: http://arxiv.org/abs/2409.17146\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Steiner et al., \u0026ldquo;PaliGemma 2: A Family of Versatile VLMs for Transfer,\u0026rdquo; Dec. 04, 2024, arXiv: arXiv:2412.03555. doi: 10.48550/arXiv.2412.03555.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nW. Hong et al., \u0026ldquo;CogVLM2: Visual Language Models for Image and Video Understanding,\u0026rdquo; Aug. 29, 2024, arXiv: arXiv:2408.16500. Accessed: Aug. 30, 2024. [Online]. Available: http://arxiv.org/abs/2408.16500\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Liu et al., \u0026ldquo;NVILA: Efficient Frontier Visual Language Models,\u0026rdquo; Dec. 05, 2024, arXiv: arXiv:2412.04468. doi: 10.48550/arXiv.2412.04468.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ouali et al., \u0026ldquo;Discriminative Fine-tuning of LVLMs,\u0026rdquo; Dec. 05, 2024, arXiv: arXiv:2412.04378. doi: 10.48550/arXiv.2412.04378.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Wang et al., \u0026ldquo;Qwen2-VL: Enhancing Vision-Language Model\u0026rsquo;s Perception of the World at Any Resolution,\u0026rdquo; Sep. 18, 2024, arXiv: arXiv:2409.12191. Accessed: Sep. 19, 2024. [Online]. Available: http://arxiv.org/abs/2409.12191\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. Liu, C. Li, Q. Wu, and Y. J. Lee, \u0026ldquo;Visual Instruction Tuning,\u0026rdquo; Dec. 11, 2023, arXiv: arXiv:2304.08485. Accessed: Jun. 28, 2024. [Online]. Available: http://arxiv.org/abs/2304.08485\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Li et al., \u0026ldquo;LLaVA-OneVision: Easy Visual Task Transfer,\u0026rdquo; Aug. 06, 2024, arXiv: arXiv:2408.03326. Accessed: Aug. 07, 2024. [Online]. Available: http://arxiv.org/abs/2408.03326\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; ","permalink":"http://localhost:1313/posts/mllms/","summary":"\u003cp\u003eThere exists a class of models whose \u003cem\u003einputs are text prompts + images or video. Their outputs are text.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eExample:  \u0026ldquo;Explain the joke in this tweet. Be concise.\u0026rdquo;\n\u003cfigure\u003e\n    \u003cimg loading=\"lazy\" src=\"/images/server-joke.webp\"\n         alt=\"home-server-joke\" width=\"600\"/\u003e \n\u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003eAnswer, courtesy of GPT4o:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe joke humorously compares \u0026ldquo;the talk\u0026rdquo; about sensitive topics with explaining to kids why there\u0026rsquo;s a server at home. The mock children\u0026rsquo;s book title exaggerates the idea, poking fun at tech enthusiasts whose home servers are significant enough to require a formal explanation to their kids.\u003c/p\u003e","title":"MLLMs, VLMs, LVLMs, LMMs..."},{"content":"\nI’m a machine learning engineer at Zoox, training and shipping foundation models for robotic perception.\nPreviously, I worked at Cobot as the first perception hire and second SWE. I did zero-to-one prototyping, evaluated sensor suites, designed the perception system, and implemented initial prototypes.\nPrior to that, I was a research scientist at Perceptive Automata, training models to help AVs understand human behavior.\nIn my academic life, I did my PhD in Neuroscience at Harvard, building novel computer vision pipelines for analyzing animal behavior in Chris Harvey’s lab.\nFind me online Twitter/X Bluesky LinkedIn GitHub Google Scholar ","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003e\u003cimg alt=\"Profile picture\" loading=\"lazy\" src=\"/images/Jim_Green.webp\"\u003e\u003c/p\u003e\n\u003cp\u003eI’m a machine learning engineer at \u003ca href=\"https://www.zoox.com\"\u003eZoox\u003c/a\u003e, training and shipping foundation models for robotic perception.\u003c/p\u003e\n\u003cp\u003ePreviously, I worked at \u003ca href=\"http://www.co.bot\"\u003eCobot\u003c/a\u003e as the first perception hire and second SWE. I did zero-to-one prototyping, evaluated sensor suites, designed the perception system, and implemented  initial prototypes.\u003c/p\u003e\n\u003cp\u003ePrior to that, I was a research scientist at Perceptive Automata, training models to help AVs understand human behavior.\u003c/p\u003e\n\u003cp\u003eIn my academic life, I did my PhD in Neuroscience at Harvard, building novel computer vision pipelines for analyzing animal behavior in Chris Harvey’s lab.\u003c/p\u003e","title":"About"}]